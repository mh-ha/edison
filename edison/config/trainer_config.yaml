accelerator: "gpu"  # str | Accelerator = "auto",
strategy: "auto"  # str | Strategy = "auto",
devices: "auto"  # List[int] | str | int = "auto",
num_nodes: 1  # int = 1,
precision: null  # _PRECISION_INPUT | None = None,
logger: null  # Logger | Iterable[Logger] | bool | None = None,
callbacks: null  # List[Callback] | Callback | None = None,
fast_dev_run: False  # int | bool = False,
max_epochs: 5  # int | None = None,
min_epochs: null  # int | None = None,
max_steps: -1  # int = -1,
min_steps: null  # int | None = None,
max_time: null  # str | timedelta | Dict[str, int] | None = None,
limit_train_batches: null  # int | float | None = None,
limit_val_batches: null  # int | float | None = None,
limit_test_batches: null  # int | float | None = None,
limit_predict_batches: null  # int | float | None = None,
overfit_batches: 0  # int | float = 0,
val_check_interval: null  # int | float | None = None,
check_val_every_n_epoch: 0  # int | None = 1,
num_sanity_val_steps: null  # int | None = None,
log_every_n_steps: null  # int | None = None,
enable_checkpointing: null  # bool | None = None,
enable_progress_bar: null  # bool | None = None,
enable_model_summary: null  # bool | None = None,
accumulate_grad_batches: 4  # int = 1,
gradient_clip_val: null  # int | float | None = None,
gradient_clip_algorithm: null  # str | None = None,
deterministic: null  # bool | Literal['warn'] | None = None,
benchmark: null  # bool | None = None,
inference_mode: True  # bool = True,
use_distributed_sampler: True  # bool = True,
profiler: null  # Profiler | str | None = None,
detect_anomaly: False  # bool = False,
barebones: False  # bool = False,
plugins: null  # _PLUGIN_INPUT | List[_PLUGIN_INPUT] | None = None,
sync_batchnorm: False  # bool = False,
reload_dataloaders_every_n_epochs: 0  # int = 0,
default_root_dir: null  # _PATH | None = None
