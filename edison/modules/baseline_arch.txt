BaselineDiffusion(
  (autoencoder): EdisonAE(
    (lm): BartForConditionalGeneration(
      (model): BartModel(
        (shared): Embedding(50265, 768, padding_idx=1)
        (encoder): BartEncoder(
          (embed_tokens): Embedding(50265, 768, padding_idx=1)
          (embed_positions): BartLearnedPositionalEmbedding(1026, 768)
          (layers): ModuleList(
            (0-5): 6 x BartEncoderLayer(
              (self_attn): BartSdpaAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (activation_fn): GELUActivation()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
          (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (decoder): BartDecoder(
          (embed_tokens): Embedding(50265, 768, padding_idx=1)
          (embed_positions): BartLearnedPositionalEmbedding(1026, 768)
          (layers): ModuleList(
            (0-5): 6 x BartDecoderLayer(
              (self_attn): BartSdpaAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (activation_fn): GELUActivation()
              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (encoder_attn): BartSdpaAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
          (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (lm_head): Linear(in_features=768, out_features=50265, bias=False)
    )
    (lm_input_embeddings): Embedding(50265, 768, padding_idx=1)
    (ae): AutoEncoder(
      (perceiver_encoder): EdisonPerceiverResampler(
        (pos_emb): AbsolutePositionalEmbedding(
          (emb): Embedding(64, 768)
        )
        (layers): ModuleList(
          (0-2): 3 x ModuleList(
            (0): EdisonPerceiverAttention(
              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (norm_latents): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (query_norm): RMSNorm()
              (key_norm): RMSNorm()
              (to_q): Linear(in_features=64, out_features=768, bias=False)
              (to_kv): Linear(in_features=768, out_features=1536, bias=False)
              (latent_to_kv): Linear(in_features=64, out_features=1536, bias=False)
              (to_out): Linear(in_features=768, out_features=64, bias=True)
            )
            (1): FeedForward(
              (network): Sequential(
                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (1): Linear(in_features=64, out_features=256, bias=True)
                (2): GELU(approximate='none')
                (3): Dropout(p=0.0, inplace=False)
                (4): Linear(in_features=256, out_features=64, bias=True)
              )
            )
          )
        )
        (final_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (perceiver_decoder): Transformer(
        (pos_emb): AbsolutePositionalEmbedding(
          (emb): Embedding(32, 768)
        )
        (input_proj): Linear(in_features=64, out_features=768, bias=True)
        (layers): ModuleList(
          (0-2): 3 x ModuleList(
            (0): Attention(
              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (query_norm): RMSNorm()
              (key_norm): RMSNorm()
              (to_q): Linear(in_features=768, out_features=768, bias=False)
              (to_k): Linear(in_features=768, out_features=768, bias=False)
              (to_v): Linear(in_features=768, out_features=768, bias=False)
              (to_out): Linear(in_features=768, out_features=768, bias=True)
            )
            (1): FeedForward(
              (network): Sequential(
                (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (1): Linear(in_features=768, out_features=3072, bias=True)
                (2): GELU(approximate='none')
                (3): Dropout(p=0.0, inplace=False)
                (4): Linear(in_features=3072, out_features=768, bias=True)
              )
            )
          )
        )
        (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (diffusion_model): BaselineDiffusionLayer(
    (encoder): BaselineEncoder(
      (layers): ModuleList(
        (0-10): 11 x ModuleList(
          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (1): Attention(
            (words_to_q): Linear(in_features=768, out_features=768, bias=False)
            (words_to_k): Linear(in_features=768, out_features=768, bias=False)
            (words_to_v): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.1, inplace=False)
            (to_out): Linear(in_features=768, out_features=768, bias=True)
          )
          (2): GRUGating(
            (gru): GRUCell(768, 768)
          )
          (3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (4): FeedForwardWithGLU(
            (glu_activation): GELU(approximate='none')
            (glu_linear): Linear(in_features=768, out_features=6144, bias=True)
            (ff): Sequential(
              (0): Dropout(p=0.1, inplace=False)
              (1): Linear(in_features=3072, out_features=768, bias=True)
            )
          )
          (5): TimeConditionedResidual(
            (scale_shift): ScaleShift(
              (time_mlp): Sequential(
                (0): GELU(approximate='none')
                (1): Linear(in_features=3072, out_features=1536, bias=True)
              )
            )
          )
        )
      )
      (proj_dense_connection): ModuleList(
        (0-2): 3 x Linear(in_features=1536, out_features=768, bias=True)
      )
    )
    (pos_emb): AbsolutePositionalEmbedding(
      (emb): Embedding(64, 768)
    )
    (time_mlp): Sequential(
      (0): SinusoidalPosEmb()
      (1): Linear(in_features=768, out_features=3072, bias=True)
      (2): GELU(approximate='none')
      (3): Linear(in_features=3072, out_features=3072, bias=True)
    )
    (time_proj): Sequential(
      (0): GELU(approximate='none')
      (1): Linear(in_features=3072, out_features=768, bias=True)
    )
    (input_proj): Linear(in_features=128, out_features=768, bias=True)
    (output_proj): Linear(in_features=768, out_features=64, bias=True)
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)