{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tests.test import test_layer, summary_layer\n",
    "from deberta.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deberta.attentions import DisentangledSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    hidden_dim=768,\n",
    "    embedding_dim=1024,\n",
    "    max_seq_len=512,\n",
    "    padding_idx=0,\n",
    "    vocab_size=30522,\n",
    "    position_biased_input=True,\n",
    "    num_heads=12,\n",
    "    num_head_dim=64,\n",
    "    layernorm_eps=1e-9,\n",
    "    hidden_dropout_prob=0.1,\n",
    "    num_hidden_layers=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DisentangledSelfAttention(\n",
       "  (query_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (key_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (value_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (relative_position_embedding): RelativePositionEmbedding(\n",
       "    (relative_position_embedding_layer): Embedding(512, 768)\n",
       "    (relative_position_query_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (relative_position_key_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (feedforward): AttentionFeedForward(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layernorm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = DisentangledSelfAttention(config)\n",
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maengmaengeeee/anaconda3/envs/edison/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([10, 512, 768])\n",
      "output type: <class 'torch.Tensor'>\n",
      "output shape: torch.Size([10, 512, 768])\n"
     ]
    }
   ],
   "source": [
    "output = test_layer(layer, (10, 512, 768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "DisentangledSelfAttention                [10, 512, 768]            --\n",
      "├─Linear: 1-1                            [10, 512, 768]            590,592\n",
      "├─Linear: 1-2                            [10, 512, 768]            590,592\n",
      "├─Linear: 1-3                            [10, 512, 768]            590,592\n",
      "├─RelativePositionEmbedding: 1-4         [10, 512, 768]            --\n",
      "│    └─Embedding: 2-1                    [512, 768]                393,216\n",
      "│    └─Linear: 2-2                       [512, 768]                590,592\n",
      "│    └─Linear: 2-3                       [512, 768]                590,592\n",
      "├─AttentionFeedForward: 1-5              [10, 512, 768]            --\n",
      "│    └─Linear: 2-4                       [10, 512, 768]            590,592\n",
      "│    └─Dropout: 2-5                      [10, 512, 768]            --\n",
      "│    └─LayerNorm: 2-6                    [10, 512, 768]            1,536\n",
      "==========================================================================================\n",
      "Total params: 3,938,304\n",
      "Trainable params: 3,938,304\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 829.73\n",
      "==========================================================================================\n",
      "Input size (MB): 15.73\n",
      "Forward/backward pass size (MB): 166.72\n",
      "Params size (MB): 15.75\n",
      "Estimated Total Size (MB): 198.21\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "summary_layer(layer, (10, 512, 768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deberta.networks import InputEmbedding\n",
    "\n",
    "layer = InputEmbedding(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([2, 512])\n",
      "output type: <class 'dict'>\n",
      "embeddings shape: torch.Size([2, 512, 768])\n",
      "position_embeddings shape: torch.Size([2, 512, 1024])\n"
     ]
    }
   ],
   "source": [
    "arr = torch.randint(0, 30522, (2, 512))\n",
    "output = test_layer(layer, input_data=arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "InputEmbedding                           [2, 512, 1024]            --\n",
      "├─Embedding: 1-1                         [2, 512, 1024]            31,254,528\n",
      "├─Embedding: 1-2                         [2, 512, 1024]            524,288\n",
      "├─Linear: 1-3                            [2, 512, 768]             786,432\n",
      "├─LayerNorm: 1-4                         [2, 512, 768]             1,536\n",
      "==========================================================================================\n",
      "Total params: 32,566,784\n",
      "Trainable params: 32,566,784\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 65.13\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 29.36\n",
      "Params size (MB): 130.27\n",
      "Estimated Total Size (MB): 159.64\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "summary_layer(layer, input_data=arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deberta.layers import RelativePositionEmbedding\n",
    "\n",
    "layer = RelativePositionEmbedding(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([2, 512, 768])\n",
      "output type: <class 'tuple'>\n",
      "output 0 shape: torch.Size([2, 512, 768])\n",
      "output 1 shape: torch.Size([2, 512, 768])\n"
     ]
    }
   ],
   "source": [
    "hidden_states = output['embeddings']\n",
    "output = test_layer(layer, input_data=hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deberta.networks import BaseNetwork\n",
    "\n",
    "embedding_layer = InputEmbedding(config)\n",
    "layer = BaseNetwork(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([10, 512])\n",
      "output type: <class 'dict'>\n",
      "embeddings shape: torch.Size([10, 512, 768])\n",
      "position_embeddings shape: torch.Size([10, 512, 1024])\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "InputEmbedding                           [10, 512, 1024]           --\n",
      "├─Embedding: 1-1                         [10, 512, 1024]           31,254,528\n",
      "├─Embedding: 1-2                         [10, 512, 1024]           524,288\n",
      "├─Linear: 1-3                            [10, 512, 768]            786,432\n",
      "├─LayerNorm: 1-4                         [10, 512, 768]            1,536\n",
      "==========================================================================================\n",
      "Total params: 32,566,784\n",
      "Trainable params: 32,566,784\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 325.67\n",
      "==========================================================================================\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 146.80\n",
      "Params size (MB): 130.27\n",
      "Estimated Total Size (MB): 277.11\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "input_data = torch.randint(0, 30522, (10, 512))\n",
    "\n",
    "output = test_layer(embedding_layer, input_data=input_data)\n",
    "summary_layer(embedding_layer, input_data=input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([10, 512, 768])\n",
      "output type: <class 'tuple'>\n",
      "output 0 shape: torch.Size([10, 512, 768])\n",
      "output 1 type: <class 'list'>\n",
      "=========================================================================================================\n",
      "Layer (type:depth-idx)                                  Output Shape              Param #\n",
      "=========================================================================================================\n",
      "BaseNetwork                                             [10, 512, 768]            --\n",
      "├─ModuleList: 1-1                                       --                        --\n",
      "│    └─TransformerBlock: 2-1                            [10, 512, 768]            --\n",
      "│    │    └─DisentangledSelfAttention: 3-1              [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-1                            [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-2                            [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-3                            [10, 512, 768]            590,592\n",
      "│    │    │    └─RelativePositionEmbedding: 4-4         [10, 512, 768]            --\n",
      "│    │    │    │    └─Embedding: 5-1                    [512, 768]                393,216\n",
      "│    │    │    │    └─Linear: 5-2                       [512, 768]                590,592\n",
      "│    │    │    │    └─Linear: 5-3                       [512, 768]                590,592\n",
      "│    │    │    └─AttentionFeedForward: 4-5              [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-4                       [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Dropout: 5-5                      [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-6                    [10, 512, 768]            1,536\n",
      "│    │    └─TransformerFeedForward: 3-2                 [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-6                            [10, 512, 3072]           2,362,368\n",
      "│    │    │    └─GELU: 4-7                              [10, 512, 3072]           --\n",
      "│    │    │    └─Linear: 4-8                            [10, 512, 768]            2,360,064\n",
      "│    │    │    └─Dropout: 4-9                           [10, 512, 768]            --\n",
      "│    │    │    └─LayerNorm: 4-10                        [10, 512, 768]            1,536\n",
      "│    └─TransformerBlock: 2-2                            [10, 512, 768]            --\n",
      "│    │    └─DisentangledSelfAttention: 3-3              [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-11                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-12                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-13                           [10, 512, 768]            590,592\n",
      "│    │    │    └─RelativePositionEmbedding: 4-14        [10, 512, 768]            --\n",
      "│    │    │    │    └─Embedding: 5-7                    [512, 768]                393,216\n",
      "│    │    │    │    └─Linear: 5-8                       [512, 768]                590,592\n",
      "│    │    │    │    └─Linear: 5-9                       [512, 768]                590,592\n",
      "│    │    │    └─AttentionFeedForward: 4-15             [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-10                      [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Dropout: 5-11                     [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-12                   [10, 512, 768]            1,536\n",
      "│    │    └─TransformerFeedForward: 3-4                 [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-16                           [10, 512, 3072]           2,362,368\n",
      "│    │    │    └─GELU: 4-17                             [10, 512, 3072]           --\n",
      "│    │    │    └─Linear: 4-18                           [10, 512, 768]            2,360,064\n",
      "│    │    │    └─Dropout: 4-19                          [10, 512, 768]            --\n",
      "│    │    │    └─LayerNorm: 4-20                        [10, 512, 768]            1,536\n",
      "│    └─TransformerBlock: 2-3                            [10, 512, 768]            --\n",
      "│    │    └─DisentangledSelfAttention: 3-5              [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-21                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-22                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-23                           [10, 512, 768]            590,592\n",
      "│    │    │    └─RelativePositionEmbedding: 4-24        [10, 512, 768]            --\n",
      "│    │    │    │    └─Embedding: 5-13                   [512, 768]                393,216\n",
      "│    │    │    │    └─Linear: 5-14                      [512, 768]                590,592\n",
      "│    │    │    │    └─Linear: 5-15                      [512, 768]                590,592\n",
      "│    │    │    └─AttentionFeedForward: 4-25             [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-16                      [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Dropout: 5-17                     [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-18                   [10, 512, 768]            1,536\n",
      "│    │    └─TransformerFeedForward: 3-6                 [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-26                           [10, 512, 3072]           2,362,368\n",
      "│    │    │    └─GELU: 4-27                             [10, 512, 3072]           --\n",
      "│    │    │    └─Linear: 4-28                           [10, 512, 768]            2,360,064\n",
      "│    │    │    └─Dropout: 4-29                          [10, 512, 768]            --\n",
      "│    │    │    └─LayerNorm: 4-30                        [10, 512, 768]            1,536\n",
      "│    └─TransformerBlock: 2-4                            [10, 512, 768]            --\n",
      "│    │    └─DisentangledSelfAttention: 3-7              [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-31                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-32                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-33                           [10, 512, 768]            590,592\n",
      "│    │    │    └─RelativePositionEmbedding: 4-34        [10, 512, 768]            --\n",
      "│    │    │    │    └─Embedding: 5-19                   [512, 768]                393,216\n",
      "│    │    │    │    └─Linear: 5-20                      [512, 768]                590,592\n",
      "│    │    │    │    └─Linear: 5-21                      [512, 768]                590,592\n",
      "│    │    │    └─AttentionFeedForward: 4-35             [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-22                      [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Dropout: 5-23                     [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-24                   [10, 512, 768]            1,536\n",
      "│    │    └─TransformerFeedForward: 3-8                 [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-36                           [10, 512, 3072]           2,362,368\n",
      "│    │    │    └─GELU: 4-37                             [10, 512, 3072]           --\n",
      "│    │    │    └─Linear: 4-38                           [10, 512, 768]            2,360,064\n",
      "│    │    │    └─Dropout: 4-39                          [10, 512, 768]            --\n",
      "│    │    │    └─LayerNorm: 4-40                        [10, 512, 768]            1,536\n",
      "│    └─TransformerBlock: 2-5                            [10, 512, 768]            --\n",
      "│    │    └─DisentangledSelfAttention: 3-9              [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-41                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-42                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-43                           [10, 512, 768]            590,592\n",
      "│    │    │    └─RelativePositionEmbedding: 4-44        [10, 512, 768]            --\n",
      "│    │    │    │    └─Embedding: 5-25                   [512, 768]                393,216\n",
      "│    │    │    │    └─Linear: 5-26                      [512, 768]                590,592\n",
      "│    │    │    │    └─Linear: 5-27                      [512, 768]                590,592\n",
      "│    │    │    └─AttentionFeedForward: 4-45             [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-28                      [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Dropout: 5-29                     [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-30                   [10, 512, 768]            1,536\n",
      "│    │    └─TransformerFeedForward: 3-10                [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-46                           [10, 512, 3072]           2,362,368\n",
      "│    │    │    └─GELU: 4-47                             [10, 512, 3072]           --\n",
      "│    │    │    └─Linear: 4-48                           [10, 512, 768]            2,360,064\n",
      "│    │    │    └─Dropout: 4-49                          [10, 512, 768]            --\n",
      "│    │    │    └─LayerNorm: 4-50                        [10, 512, 768]            1,536\n",
      "│    └─TransformerBlock: 2-6                            [10, 512, 768]            --\n",
      "│    │    └─DisentangledSelfAttention: 3-11             [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-51                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-52                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-53                           [10, 512, 768]            590,592\n",
      "│    │    │    └─RelativePositionEmbedding: 4-54        [10, 512, 768]            --\n",
      "│    │    │    │    └─Embedding: 5-31                   [512, 768]                393,216\n",
      "│    │    │    │    └─Linear: 5-32                      [512, 768]                590,592\n",
      "│    │    │    │    └─Linear: 5-33                      [512, 768]                590,592\n",
      "│    │    │    └─AttentionFeedForward: 4-55             [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-34                      [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Dropout: 5-35                     [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-36                   [10, 512, 768]            1,536\n",
      "│    │    └─TransformerFeedForward: 3-12                [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-56                           [10, 512, 3072]           2,362,368\n",
      "│    │    │    └─GELU: 4-57                             [10, 512, 3072]           --\n",
      "│    │    │    └─Linear: 4-58                           [10, 512, 768]            2,360,064\n",
      "│    │    │    └─Dropout: 4-59                          [10, 512, 768]            --\n",
      "│    │    │    └─LayerNorm: 4-60                        [10, 512, 768]            1,536\n",
      "│    └─TransformerBlock: 2-7                            [10, 512, 768]            --\n",
      "│    │    └─DisentangledSelfAttention: 3-13             [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-61                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-62                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-63                           [10, 512, 768]            590,592\n",
      "│    │    │    └─RelativePositionEmbedding: 4-64        [10, 512, 768]            --\n",
      "│    │    │    │    └─Embedding: 5-37                   [512, 768]                393,216\n",
      "│    │    │    │    └─Linear: 5-38                      [512, 768]                590,592\n",
      "│    │    │    │    └─Linear: 5-39                      [512, 768]                590,592\n",
      "│    │    │    └─AttentionFeedForward: 4-65             [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-40                      [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Dropout: 5-41                     [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-42                   [10, 512, 768]            1,536\n",
      "│    │    └─TransformerFeedForward: 3-14                [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-66                           [10, 512, 3072]           2,362,368\n",
      "│    │    │    └─GELU: 4-67                             [10, 512, 3072]           --\n",
      "│    │    │    └─Linear: 4-68                           [10, 512, 768]            2,360,064\n",
      "│    │    │    └─Dropout: 4-69                          [10, 512, 768]            --\n",
      "│    │    │    └─LayerNorm: 4-70                        [10, 512, 768]            1,536\n",
      "│    └─TransformerBlock: 2-8                            [10, 512, 768]            --\n",
      "│    │    └─DisentangledSelfAttention: 3-15             [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-71                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-72                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-73                           [10, 512, 768]            590,592\n",
      "│    │    │    └─RelativePositionEmbedding: 4-74        [10, 512, 768]            --\n",
      "│    │    │    │    └─Embedding: 5-43                   [512, 768]                393,216\n",
      "│    │    │    │    └─Linear: 5-44                      [512, 768]                590,592\n",
      "│    │    │    │    └─Linear: 5-45                      [512, 768]                590,592\n",
      "│    │    │    └─AttentionFeedForward: 4-75             [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-46                      [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Dropout: 5-47                     [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-48                   [10, 512, 768]            1,536\n",
      "│    │    └─TransformerFeedForward: 3-16                [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-76                           [10, 512, 3072]           2,362,368\n",
      "│    │    │    └─GELU: 4-77                             [10, 512, 3072]           --\n",
      "│    │    │    └─Linear: 4-78                           [10, 512, 768]            2,360,064\n",
      "│    │    │    └─Dropout: 4-79                          [10, 512, 768]            --\n",
      "│    │    │    └─LayerNorm: 4-80                        [10, 512, 768]            1,536\n",
      "│    └─TransformerBlock: 2-9                            [10, 512, 768]            --\n",
      "│    │    └─DisentangledSelfAttention: 3-17             [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-81                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-82                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-83                           [10, 512, 768]            590,592\n",
      "│    │    │    └─RelativePositionEmbedding: 4-84        [10, 512, 768]            --\n",
      "│    │    │    │    └─Embedding: 5-49                   [512, 768]                393,216\n",
      "│    │    │    │    └─Linear: 5-50                      [512, 768]                590,592\n",
      "│    │    │    │    └─Linear: 5-51                      [512, 768]                590,592\n",
      "│    │    │    └─AttentionFeedForward: 4-85             [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-52                      [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Dropout: 5-53                     [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-54                   [10, 512, 768]            1,536\n",
      "│    │    └─TransformerFeedForward: 3-18                [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-86                           [10, 512, 3072]           2,362,368\n",
      "│    │    │    └─GELU: 4-87                             [10, 512, 3072]           --\n",
      "│    │    │    └─Linear: 4-88                           [10, 512, 768]            2,360,064\n",
      "│    │    │    └─Dropout: 4-89                          [10, 512, 768]            --\n",
      "│    │    │    └─LayerNorm: 4-90                        [10, 512, 768]            1,536\n",
      "│    └─TransformerBlock: 2-10                           [10, 512, 768]            --\n",
      "│    │    └─DisentangledSelfAttention: 3-19             [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-91                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-92                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-93                           [10, 512, 768]            590,592\n",
      "│    │    │    └─RelativePositionEmbedding: 4-94        [10, 512, 768]            --\n",
      "│    │    │    │    └─Embedding: 5-55                   [512, 768]                393,216\n",
      "│    │    │    │    └─Linear: 5-56                      [512, 768]                590,592\n",
      "│    │    │    │    └─Linear: 5-57                      [512, 768]                590,592\n",
      "│    │    │    └─AttentionFeedForward: 4-95             [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-58                      [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Dropout: 5-59                     [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-60                   [10, 512, 768]            1,536\n",
      "│    │    └─TransformerFeedForward: 3-20                [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-96                           [10, 512, 3072]           2,362,368\n",
      "│    │    │    └─GELU: 4-97                             [10, 512, 3072]           --\n",
      "│    │    │    └─Linear: 4-98                           [10, 512, 768]            2,360,064\n",
      "│    │    │    └─Dropout: 4-99                          [10, 512, 768]            --\n",
      "│    │    │    └─LayerNorm: 4-100                       [10, 512, 768]            1,536\n",
      "│    └─TransformerBlock: 2-11                           [10, 512, 768]            --\n",
      "│    │    └─DisentangledSelfAttention: 3-21             [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-101                          [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-102                          [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-103                          [10, 512, 768]            590,592\n",
      "│    │    │    └─RelativePositionEmbedding: 4-104       [10, 512, 768]            --\n",
      "│    │    │    │    └─Embedding: 5-61                   [512, 768]                393,216\n",
      "│    │    │    │    └─Linear: 5-62                      [512, 768]                590,592\n",
      "│    │    │    │    └─Linear: 5-63                      [512, 768]                590,592\n",
      "│    │    │    └─AttentionFeedForward: 4-105            [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-64                      [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Dropout: 5-65                     [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-66                   [10, 512, 768]            1,536\n",
      "│    │    └─TransformerFeedForward: 3-22                [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-106                          [10, 512, 3072]           2,362,368\n",
      "│    │    │    └─GELU: 4-107                            [10, 512, 3072]           --\n",
      "│    │    │    └─Linear: 4-108                          [10, 512, 768]            2,360,064\n",
      "│    │    │    └─Dropout: 4-109                         [10, 512, 768]            --\n",
      "│    │    │    └─LayerNorm: 4-110                       [10, 512, 768]            1,536\n",
      "│    └─TransformerBlock: 2-12                           [10, 512, 768]            --\n",
      "│    │    └─DisentangledSelfAttention: 3-23             [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-111                          [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-112                          [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-113                          [10, 512, 768]            590,592\n",
      "│    │    │    └─RelativePositionEmbedding: 4-114       [10, 512, 768]            --\n",
      "│    │    │    │    └─Embedding: 5-67                   [512, 768]                393,216\n",
      "│    │    │    │    └─Linear: 5-68                      [512, 768]                590,592\n",
      "│    │    │    │    └─Linear: 5-69                      [512, 768]                590,592\n",
      "│    │    │    └─AttentionFeedForward: 4-115            [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-70                      [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Dropout: 5-71                     [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-72                   [10, 512, 768]            1,536\n",
      "│    │    └─TransformerFeedForward: 3-24                [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-116                          [10, 512, 3072]           2,362,368\n",
      "│    │    │    └─GELU: 4-117                            [10, 512, 3072]           --\n",
      "│    │    │    └─Linear: 4-118                          [10, 512, 768]            2,360,064\n",
      "│    │    │    └─Dropout: 4-119                         [10, 512, 768]            --\n",
      "│    │    │    └─LayerNorm: 4-120                       [10, 512, 768]            1,536\n",
      "=========================================================================================================\n",
      "Total params: 103,947,264\n",
      "Trainable params: 103,947,264\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 10.52\n",
      "=========================================================================================================\n",
      "Input size (MB): 15.73\n",
      "Forward/backward pass size (MB): 4265.61\n",
      "Params size (MB): 415.79\n",
      "Estimated Total Size (MB): 4697.12\n",
      "=========================================================================================================\n"
     ]
    }
   ],
   "source": [
    "inputs = output['embeddings']\n",
    "output = test_layer(layer, input_data=inputs)\n",
    "summary_layer(layer, input_data=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512, 768])\n",
      "torch.Size([10, 512, 768])\n",
      "torch.Size([10, 512, 768])\n",
      "torch.Size([10, 512, 768])\n",
      "torch.Size([10, 512, 768])\n",
      "torch.Size([10, 512, 768])\n",
      "torch.Size([10, 512, 768])\n",
      "torch.Size([10, 512, 768])\n",
      "torch.Size([10, 512, 768])\n",
      "torch.Size([10, 512, 768])\n",
      "torch.Size([10, 512, 768])\n",
      "torch.Size([10, 512, 768])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(output[1])):\n",
    "    print(output[1][i].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edison",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
