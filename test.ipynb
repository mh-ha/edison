{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tests.test import test_layer, summary_layer\n",
    "from deberta.config import Config\n",
    "\n",
    "config = Config(\n",
    "    hidden_dim=768,\n",
    "    embedding_dim=1024,\n",
    "    max_seq_len=512,\n",
    "    padding_idx=0,\n",
    "    vocab_size=128001,\n",
    "    absolute_position_biased_input=True,\n",
    "    num_heads=12,\n",
    "    num_head_dim=64,\n",
    "    layernorm_eps=1e-9,\n",
    "    hidden_dropout_prob=0.1,\n",
    "    num_hidden_layers=12,\n",
    "    device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deberta.attentions import DisentangledSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_device(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DisentangledSelfAttention(\n",
       "  (query_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (key_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (value_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (relative_position_embedding): RelativePositionEmbedding(\n",
       "    (relative_position_embedding_layer): Embedding(512, 768)\n",
       "    (relative_position_query_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (relative_position_key_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (feedforward): AttentionFeedForward(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layernorm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = DisentangledSelfAttention(config)\n",
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = test_layer(layer, (10, 512, 768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "DisentangledSelfAttention                [10, 512, 768]            --\n",
      "├─Linear: 1-1                            [10, 512, 768]            590,592\n",
      "├─Linear: 1-2                            [10, 512, 768]            590,592\n",
      "├─Linear: 1-3                            [10, 512, 768]            590,592\n",
      "├─RelativePositionEmbedding: 1-4         [10, 512, 768]            --\n",
      "│    └─Embedding: 2-1                    [512, 768]                393,216\n",
      "│    └─Linear: 2-2                       [512, 768]                590,592\n",
      "│    └─Linear: 2-3                       [512, 768]                590,592\n",
      "├─AttentionFeedForward: 1-5              [10, 512, 768]            --\n",
      "│    └─Linear: 2-4                       [10, 512, 768]            590,592\n",
      "│    └─Dropout: 2-5                      [10, 512, 768]            --\n",
      "│    └─LayerNorm: 2-6                    [10, 512, 768]            1,536\n",
      "==========================================================================================\n",
      "Total params: 3,938,304\n",
      "Trainable params: 3,938,304\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 829.73\n",
      "==========================================================================================\n",
      "Input size (MB): 15.73\n",
      "Forward/backward pass size (MB): 166.72\n",
      "Params size (MB): 15.75\n",
      "Estimated Total Size (MB): 198.21\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "summary_layer(layer, (10, 512, 768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deberta.networks import InputEmbedding\n",
    "\n",
    "layer = InputEmbedding(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([2, 512])\n",
      "output type: <class 'dict'>\n",
      "embeddings shape: torch.Size([2, 512, 768])\n",
      "position_embeddings shape: torch.Size([2, 512, 768])\n"
     ]
    }
   ],
   "source": [
    "arr = torch.randint(0, 30522, (2, 512))\n",
    "output = test_layer(layer, input_data=arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "InputEmbedding                           [2, 512, 768]             --\n",
      "├─Embedding: 1-1                         [2, 512, 1024]            31,254,528\n",
      "├─Embedding: 1-2                         [2, 512, 1024]            524,288\n",
      "├─Linear: 1-3                            [2, 512, 768]             786,432\n",
      "├─Linear: 1-4                            [2, 512, 768]             786,432\n",
      "├─LayerNorm: 1-5                         [2, 512, 768]             1,536\n",
      "==========================================================================================\n",
      "Total params: 33,353,216\n",
      "Trainable params: 33,353,216\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 66.71\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 35.65\n",
      "Params size (MB): 133.41\n",
      "Estimated Total Size (MB): 169.07\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "summary_layer(layer, input_data=arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deberta.layers import RelativePositionEmbedding\n",
    "\n",
    "layer = RelativePositionEmbedding(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([2, 512, 768])\n",
      "output type: <class 'tuple'>\n",
      "output 0 shape: torch.Size([2, 512, 768])\n",
      "output 1 shape: torch.Size([2, 512, 768])\n"
     ]
    }
   ],
   "source": [
    "hidden_states = output['embeddings']\n",
    "output = test_layer(layer, input_data=hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deberta.networks import BaseNetwork\n",
    "\n",
    "embedding_layer = InputEmbedding(config)\n",
    "layer = BaseNetwork(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([10, 512])\n",
      "output type: <class 'dict'>\n",
      "embeddings shape: torch.Size([10, 512, 768])\n",
      "position_embeddings shape: torch.Size([10, 512, 768])\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "InputEmbedding                           [10, 512, 768]            --\n",
      "├─Embedding: 1-1                         [10, 512, 1024]           31,254,528\n",
      "├─Embedding: 1-2                         [10, 512, 1024]           524,288\n",
      "├─Linear: 1-3                            [10, 512, 768]            786,432\n",
      "├─Linear: 1-4                            [10, 512, 768]            786,432\n",
      "├─LayerNorm: 1-5                         [10, 512, 768]            1,536\n",
      "==========================================================================================\n",
      "Total params: 33,353,216\n",
      "Trainable params: 33,353,216\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 333.53\n",
      "==========================================================================================\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 178.26\n",
      "Params size (MB): 133.41\n",
      "Estimated Total Size (MB): 311.71\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "input_data = torch.randint(0, 30522, (10, 512))\n",
    "\n",
    "output = test_layer(embedding_layer, input_data=input_data)\n",
    "summary_layer(embedding_layer, input_data=input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([10, 512, 768])\n",
      "output type: <class 'tuple'>\n",
      "output 0 shape: torch.Size([10, 512, 768])\n",
      "output 1 type: <class 'list'>\n",
      "=========================================================================================================\n",
      "Layer (type:depth-idx)                                  Output Shape              Param #\n",
      "=========================================================================================================\n",
      "BaseNetwork                                             [10, 512, 768]            --\n",
      "├─ModuleList: 1-1                                       --                        --\n",
      "│    └─TransformerBlock: 2-1                            [10, 512, 768]            --\n",
      "│    │    └─DisentangledSelfAttention: 3-1              [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-1                            [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-2                            [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-3                            [10, 512, 768]            590,592\n",
      "│    │    │    └─RelativePositionEmbedding: 4-4         [10, 512, 768]            --\n",
      "│    │    │    │    └─Embedding: 5-1                    [512, 768]                393,216\n",
      "│    │    │    │    └─Linear: 5-2                       [512, 768]                590,592\n",
      "│    │    │    │    └─Linear: 5-3                       [512, 768]                590,592\n",
      "│    │    │    └─AttentionFeedForward: 4-5              [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-4                       [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Dropout: 5-5                      [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-6                    [10, 512, 768]            1,536\n",
      "│    │    └─TransformerFeedForward: 3-2                 [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-6                            [10, 512, 3072]           2,362,368\n",
      "│    │    │    └─GELU: 4-7                              [10, 512, 3072]           --\n",
      "│    │    │    └─Linear: 4-8                            [10, 512, 768]            2,360,064\n",
      "│    │    │    └─Dropout: 4-9                           [10, 512, 768]            --\n",
      "│    │    │    └─LayerNorm: 4-10                        [10, 512, 768]            1,536\n",
      "│    └─TransformerBlock: 2-2                            [10, 512, 768]            --\n",
      "│    │    └─DisentangledSelfAttention: 3-3              [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-11                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-12                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-13                           [10, 512, 768]            590,592\n",
      "│    │    │    └─RelativePositionEmbedding: 4-14        [10, 512, 768]            --\n",
      "│    │    │    │    └─Embedding: 5-7                    [512, 768]                393,216\n",
      "│    │    │    │    └─Linear: 5-8                       [512, 768]                590,592\n",
      "│    │    │    │    └─Linear: 5-9                       [512, 768]                590,592\n",
      "│    │    │    └─AttentionFeedForward: 4-15             [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-10                      [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Dropout: 5-11                     [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-12                   [10, 512, 768]            1,536\n",
      "│    │    └─TransformerFeedForward: 3-4                 [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-16                           [10, 512, 3072]           2,362,368\n",
      "│    │    │    └─GELU: 4-17                             [10, 512, 3072]           --\n",
      "│    │    │    └─Linear: 4-18                           [10, 512, 768]            2,360,064\n",
      "│    │    │    └─Dropout: 4-19                          [10, 512, 768]            --\n",
      "│    │    │    └─LayerNorm: 4-20                        [10, 512, 768]            1,536\n",
      "│    └─TransformerBlock: 2-3                            [10, 512, 768]            --\n",
      "│    │    └─DisentangledSelfAttention: 3-5              [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-21                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-22                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-23                           [10, 512, 768]            590,592\n",
      "│    │    │    └─RelativePositionEmbedding: 4-24        [10, 512, 768]            --\n",
      "│    │    │    │    └─Embedding: 5-13                   [512, 768]                393,216\n",
      "│    │    │    │    └─Linear: 5-14                      [512, 768]                590,592\n",
      "│    │    │    │    └─Linear: 5-15                      [512, 768]                590,592\n",
      "│    │    │    └─AttentionFeedForward: 4-25             [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-16                      [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Dropout: 5-17                     [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-18                   [10, 512, 768]            1,536\n",
      "│    │    └─TransformerFeedForward: 3-6                 [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-26                           [10, 512, 3072]           2,362,368\n",
      "│    │    │    └─GELU: 4-27                             [10, 512, 3072]           --\n",
      "│    │    │    └─Linear: 4-28                           [10, 512, 768]            2,360,064\n",
      "│    │    │    └─Dropout: 4-29                          [10, 512, 768]            --\n",
      "│    │    │    └─LayerNorm: 4-30                        [10, 512, 768]            1,536\n",
      "│    └─TransformerBlock: 2-4                            [10, 512, 768]            --\n",
      "│    │    └─DisentangledSelfAttention: 3-7              [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-31                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-32                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-33                           [10, 512, 768]            590,592\n",
      "│    │    │    └─RelativePositionEmbedding: 4-34        [10, 512, 768]            --\n",
      "│    │    │    │    └─Embedding: 5-19                   [512, 768]                393,216\n",
      "│    │    │    │    └─Linear: 5-20                      [512, 768]                590,592\n",
      "│    │    │    │    └─Linear: 5-21                      [512, 768]                590,592\n",
      "│    │    │    └─AttentionFeedForward: 4-35             [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-22                      [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Dropout: 5-23                     [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-24                   [10, 512, 768]            1,536\n",
      "│    │    └─TransformerFeedForward: 3-8                 [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-36                           [10, 512, 3072]           2,362,368\n",
      "│    │    │    └─GELU: 4-37                             [10, 512, 3072]           --\n",
      "│    │    │    └─Linear: 4-38                           [10, 512, 768]            2,360,064\n",
      "│    │    │    └─Dropout: 4-39                          [10, 512, 768]            --\n",
      "│    │    │    └─LayerNorm: 4-40                        [10, 512, 768]            1,536\n",
      "│    └─TransformerBlock: 2-5                            [10, 512, 768]            --\n",
      "│    │    └─DisentangledSelfAttention: 3-9              [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-41                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-42                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-43                           [10, 512, 768]            590,592\n",
      "│    │    │    └─RelativePositionEmbedding: 4-44        [10, 512, 768]            --\n",
      "│    │    │    │    └─Embedding: 5-25                   [512, 768]                393,216\n",
      "│    │    │    │    └─Linear: 5-26                      [512, 768]                590,592\n",
      "│    │    │    │    └─Linear: 5-27                      [512, 768]                590,592\n",
      "│    │    │    └─AttentionFeedForward: 4-45             [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-28                      [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Dropout: 5-29                     [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-30                   [10, 512, 768]            1,536\n",
      "│    │    └─TransformerFeedForward: 3-10                [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-46                           [10, 512, 3072]           2,362,368\n",
      "│    │    │    └─GELU: 4-47                             [10, 512, 3072]           --\n",
      "│    │    │    └─Linear: 4-48                           [10, 512, 768]            2,360,064\n",
      "│    │    │    └─Dropout: 4-49                          [10, 512, 768]            --\n",
      "│    │    │    └─LayerNorm: 4-50                        [10, 512, 768]            1,536\n",
      "│    └─TransformerBlock: 2-6                            [10, 512, 768]            --\n",
      "│    │    └─DisentangledSelfAttention: 3-11             [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-51                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-52                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-53                           [10, 512, 768]            590,592\n",
      "│    │    │    └─RelativePositionEmbedding: 4-54        [10, 512, 768]            --\n",
      "│    │    │    │    └─Embedding: 5-31                   [512, 768]                393,216\n",
      "│    │    │    │    └─Linear: 5-32                      [512, 768]                590,592\n",
      "│    │    │    │    └─Linear: 5-33                      [512, 768]                590,592\n",
      "│    │    │    └─AttentionFeedForward: 4-55             [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-34                      [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Dropout: 5-35                     [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-36                   [10, 512, 768]            1,536\n",
      "│    │    └─TransformerFeedForward: 3-12                [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-56                           [10, 512, 3072]           2,362,368\n",
      "│    │    │    └─GELU: 4-57                             [10, 512, 3072]           --\n",
      "│    │    │    └─Linear: 4-58                           [10, 512, 768]            2,360,064\n",
      "│    │    │    └─Dropout: 4-59                          [10, 512, 768]            --\n",
      "│    │    │    └─LayerNorm: 4-60                        [10, 512, 768]            1,536\n",
      "│    └─TransformerBlock: 2-7                            [10, 512, 768]            --\n",
      "│    │    └─DisentangledSelfAttention: 3-13             [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-61                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-62                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-63                           [10, 512, 768]            590,592\n",
      "│    │    │    └─RelativePositionEmbedding: 4-64        [10, 512, 768]            --\n",
      "│    │    │    │    └─Embedding: 5-37                   [512, 768]                393,216\n",
      "│    │    │    │    └─Linear: 5-38                      [512, 768]                590,592\n",
      "│    │    │    │    └─Linear: 5-39                      [512, 768]                590,592\n",
      "│    │    │    └─AttentionFeedForward: 4-65             [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-40                      [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Dropout: 5-41                     [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-42                   [10, 512, 768]            1,536\n",
      "│    │    └─TransformerFeedForward: 3-14                [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-66                           [10, 512, 3072]           2,362,368\n",
      "│    │    │    └─GELU: 4-67                             [10, 512, 3072]           --\n",
      "│    │    │    └─Linear: 4-68                           [10, 512, 768]            2,360,064\n",
      "│    │    │    └─Dropout: 4-69                          [10, 512, 768]            --\n",
      "│    │    │    └─LayerNorm: 4-70                        [10, 512, 768]            1,536\n",
      "│    └─TransformerBlock: 2-8                            [10, 512, 768]            --\n",
      "│    │    └─DisentangledSelfAttention: 3-15             [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-71                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-72                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-73                           [10, 512, 768]            590,592\n",
      "│    │    │    └─RelativePositionEmbedding: 4-74        [10, 512, 768]            --\n",
      "│    │    │    │    └─Embedding: 5-43                   [512, 768]                393,216\n",
      "│    │    │    │    └─Linear: 5-44                      [512, 768]                590,592\n",
      "│    │    │    │    └─Linear: 5-45                      [512, 768]                590,592\n",
      "│    │    │    └─AttentionFeedForward: 4-75             [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-46                      [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Dropout: 5-47                     [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-48                   [10, 512, 768]            1,536\n",
      "│    │    └─TransformerFeedForward: 3-16                [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-76                           [10, 512, 3072]           2,362,368\n",
      "│    │    │    └─GELU: 4-77                             [10, 512, 3072]           --\n",
      "│    │    │    └─Linear: 4-78                           [10, 512, 768]            2,360,064\n",
      "│    │    │    └─Dropout: 4-79                          [10, 512, 768]            --\n",
      "│    │    │    └─LayerNorm: 4-80                        [10, 512, 768]            1,536\n",
      "│    └─TransformerBlock: 2-9                            [10, 512, 768]            --\n",
      "│    │    └─DisentangledSelfAttention: 3-17             [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-81                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-82                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-83                           [10, 512, 768]            590,592\n",
      "│    │    │    └─RelativePositionEmbedding: 4-84        [10, 512, 768]            --\n",
      "│    │    │    │    └─Embedding: 5-49                   [512, 768]                393,216\n",
      "│    │    │    │    └─Linear: 5-50                      [512, 768]                590,592\n",
      "│    │    │    │    └─Linear: 5-51                      [512, 768]                590,592\n",
      "│    │    │    └─AttentionFeedForward: 4-85             [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-52                      [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Dropout: 5-53                     [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-54                   [10, 512, 768]            1,536\n",
      "│    │    └─TransformerFeedForward: 3-18                [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-86                           [10, 512, 3072]           2,362,368\n",
      "│    │    │    └─GELU: 4-87                             [10, 512, 3072]           --\n",
      "│    │    │    └─Linear: 4-88                           [10, 512, 768]            2,360,064\n",
      "│    │    │    └─Dropout: 4-89                          [10, 512, 768]            --\n",
      "│    │    │    └─LayerNorm: 4-90                        [10, 512, 768]            1,536\n",
      "│    └─TransformerBlock: 2-10                           [10, 512, 768]            --\n",
      "│    │    └─DisentangledSelfAttention: 3-19             [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-91                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-92                           [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-93                           [10, 512, 768]            590,592\n",
      "│    │    │    └─RelativePositionEmbedding: 4-94        [10, 512, 768]            --\n",
      "│    │    │    │    └─Embedding: 5-55                   [512, 768]                393,216\n",
      "│    │    │    │    └─Linear: 5-56                      [512, 768]                590,592\n",
      "│    │    │    │    └─Linear: 5-57                      [512, 768]                590,592\n",
      "│    │    │    └─AttentionFeedForward: 4-95             [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-58                      [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Dropout: 5-59                     [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-60                   [10, 512, 768]            1,536\n",
      "│    │    └─TransformerFeedForward: 3-20                [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-96                           [10, 512, 3072]           2,362,368\n",
      "│    │    │    └─GELU: 4-97                             [10, 512, 3072]           --\n",
      "│    │    │    └─Linear: 4-98                           [10, 512, 768]            2,360,064\n",
      "│    │    │    └─Dropout: 4-99                          [10, 512, 768]            --\n",
      "│    │    │    └─LayerNorm: 4-100                       [10, 512, 768]            1,536\n",
      "│    └─TransformerBlock: 2-11                           [10, 512, 768]            --\n",
      "│    │    └─DisentangledSelfAttention: 3-21             [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-101                          [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-102                          [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-103                          [10, 512, 768]            590,592\n",
      "│    │    │    └─RelativePositionEmbedding: 4-104       [10, 512, 768]            --\n",
      "│    │    │    │    └─Embedding: 5-61                   [512, 768]                393,216\n",
      "│    │    │    │    └─Linear: 5-62                      [512, 768]                590,592\n",
      "│    │    │    │    └─Linear: 5-63                      [512, 768]                590,592\n",
      "│    │    │    └─AttentionFeedForward: 4-105            [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-64                      [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Dropout: 5-65                     [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-66                   [10, 512, 768]            1,536\n",
      "│    │    └─TransformerFeedForward: 3-22                [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-106                          [10, 512, 3072]           2,362,368\n",
      "│    │    │    └─GELU: 4-107                            [10, 512, 3072]           --\n",
      "│    │    │    └─Linear: 4-108                          [10, 512, 768]            2,360,064\n",
      "│    │    │    └─Dropout: 4-109                         [10, 512, 768]            --\n",
      "│    │    │    └─LayerNorm: 4-110                       [10, 512, 768]            1,536\n",
      "│    └─TransformerBlock: 2-12                           [10, 512, 768]            --\n",
      "│    │    └─DisentangledSelfAttention: 3-23             [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-111                          [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-112                          [10, 512, 768]            590,592\n",
      "│    │    │    └─Linear: 4-113                          [10, 512, 768]            590,592\n",
      "│    │    │    └─RelativePositionEmbedding: 4-114       [10, 512, 768]            --\n",
      "│    │    │    │    └─Embedding: 5-67                   [512, 768]                393,216\n",
      "│    │    │    │    └─Linear: 5-68                      [512, 768]                590,592\n",
      "│    │    │    │    └─Linear: 5-69                      [512, 768]                590,592\n",
      "│    │    │    └─AttentionFeedForward: 4-115            [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-70                      [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Dropout: 5-71                     [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-72                   [10, 512, 768]            1,536\n",
      "│    │    └─TransformerFeedForward: 3-24                [10, 512, 768]            --\n",
      "│    │    │    └─Linear: 4-116                          [10, 512, 3072]           2,362,368\n",
      "│    │    │    └─GELU: 4-117                            [10, 512, 3072]           --\n",
      "│    │    │    └─Linear: 4-118                          [10, 512, 768]            2,360,064\n",
      "│    │    │    └─Dropout: 4-119                         [10, 512, 768]            --\n",
      "│    │    │    └─LayerNorm: 4-120                       [10, 512, 768]            1,536\n",
      "=========================================================================================================\n",
      "Total params: 103,947,264\n",
      "Trainable params: 103,947,264\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 10.52\n",
      "=========================================================================================================\n",
      "Input size (MB): 15.73\n",
      "Forward/backward pass size (MB): 4265.61\n",
      "Params size (MB): 415.79\n",
      "Estimated Total Size (MB): 4697.12\n",
      "=========================================================================================================\n"
     ]
    }
   ],
   "source": [
    "inputs = output['embeddings']\n",
    "output = test_layer(layer, input_data=inputs)\n",
    "summary_layer(layer, input_data=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512, 768])\n",
      "torch.Size([10, 512, 768])\n",
      "torch.Size([10, 512, 768])\n",
      "torch.Size([10, 512, 768])\n",
      "torch.Size([10, 512, 768])\n",
      "torch.Size([10, 512, 768])\n",
      "torch.Size([10, 512, 768])\n",
      "torch.Size([10, 512, 768])\n",
      "torch.Size([10, 512, 768])\n",
      "torch.Size([10, 512, 768])\n",
      "torch.Size([10, 512, 768])\n",
      "torch.Size([10, 512, 768])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(output[1])):\n",
    "    print(output[1][i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deberta.networks import Generator, Discriminator\n",
    "\n",
    "generator = Generator(config)\n",
    "discriminator = Discriminator(config)\n",
    "\n",
    "inputs = torch.randint(0, config.vocab_size, (10, config.max_seq_len))\n",
    "labels = torch.randint(0, 2, (10, config.max_seq_len))\n",
    "masks = torch.randint(0, 2, (10, config.max_seq_len))\n",
    "# output = test_layer(generator, input_data=inputs)\n",
    "# summary_layer(generator, input_data=inputs)\n",
    "# output = generator(inputs, labels=labels, labels_mask=masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 512, 30522]), torch.Size([2509]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape, output[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = discriminator(inputs, labels=labels, labels_mask=masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 512, 1]), torch.Size([2509]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape, output[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0,  ..., 1, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[masks>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([10, 512])\n",
      "output type: <class 'torch.Tensor'>\n",
      "output shape: torch.Size([10, 512, 1])\n",
      "==============================================================================================================\n",
      "Layer (type:depth-idx)                                       Output Shape              Param #\n",
      "==============================================================================================================\n",
      "Discriminator                                                [10, 512, 1]              --\n",
      "├─InputEmbedding: 1-1                                        [10, 512, 768]            --\n",
      "│    └─Embedding: 2-1                                        [10, 512, 1024]           31,254,528\n",
      "│    └─Embedding: 2-2                                        [10, 512, 1024]           524,288\n",
      "│    └─Linear: 2-3                                           [10, 512, 768]            786,432\n",
      "│    └─Linear: 2-4                                           [10, 512, 768]            786,432\n",
      "│    └─LayerNorm: 2-5                                        [10, 512, 768]            1,536\n",
      "├─BaseNetwork: 1-2                                           [10, 512, 768]            --\n",
      "│    └─ModuleList: 2-7                                       --                        (recursive)\n",
      "│    │    └─TransformerBlock: 3-1                            [10, 512, 768]            --\n",
      "│    │    │    └─DisentangledSelfAttention: 4-1              [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-1                            [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Linear: 5-2                            [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Linear: 5-3                            [10, 512, 768]            590,592\n",
      "│    │    │    │    └─RelativePositionEmbedding: 5-4         [10, 512, 768]            --\n",
      "│    │    │    │    │    └─Embedding: 6-1                    [512, 768]                393,216\n",
      "│    │    │    │    │    └─Linear: 6-2                       [512, 768]                590,592\n",
      "│    │    │    │    │    └─Linear: 6-3                       [512, 768]                590,592\n",
      "│    │    │    │    └─AttentionFeedForward: 5-5              [10, 512, 768]            --\n",
      "│    │    │    │    │    └─Linear: 6-4                       [10, 512, 768]            590,592\n",
      "│    │    │    │    │    └─Dropout: 6-5                      [10, 512, 768]            --\n",
      "│    │    │    │    │    └─LayerNorm: 6-6                    [10, 512, 768]            1,536\n",
      "│    │    │    └─TransformerFeedForward: 4-2                 [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-6                            [10, 512, 3072]           2,362,368\n",
      "│    │    │    │    └─GELU: 5-7                              [10, 512, 3072]           --\n",
      "│    │    │    │    └─Linear: 5-8                            [10, 512, 768]            2,360,064\n",
      "│    │    │    │    └─Dropout: 5-9                           [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-10                        [10, 512, 768]            1,536\n",
      "│    │    └─TransformerBlock: 3-2                            [10, 512, 768]            --\n",
      "│    │    │    └─DisentangledSelfAttention: 4-3              [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-11                           [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Linear: 5-12                           [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Linear: 5-13                           [10, 512, 768]            590,592\n",
      "│    │    │    │    └─RelativePositionEmbedding: 5-14        [10, 512, 768]            --\n",
      "│    │    │    │    │    └─Embedding: 6-7                    [512, 768]                393,216\n",
      "│    │    │    │    │    └─Linear: 6-8                       [512, 768]                590,592\n",
      "│    │    │    │    │    └─Linear: 6-9                       [512, 768]                590,592\n",
      "│    │    │    │    └─AttentionFeedForward: 5-15             [10, 512, 768]            --\n",
      "│    │    │    │    │    └─Linear: 6-10                      [10, 512, 768]            590,592\n",
      "│    │    │    │    │    └─Dropout: 6-11                     [10, 512, 768]            --\n",
      "│    │    │    │    │    └─LayerNorm: 6-12                   [10, 512, 768]            1,536\n",
      "│    │    │    └─TransformerFeedForward: 4-4                 [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-16                           [10, 512, 3072]           2,362,368\n",
      "│    │    │    │    └─GELU: 5-17                             [10, 512, 3072]           --\n",
      "│    │    │    │    └─Linear: 5-18                           [10, 512, 768]            2,360,064\n",
      "│    │    │    │    └─Dropout: 5-19                          [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-20                        [10, 512, 768]            1,536\n",
      "│    │    └─TransformerBlock: 3-3                            [10, 512, 768]            --\n",
      "│    │    │    └─DisentangledSelfAttention: 4-5              [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-21                           [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Linear: 5-22                           [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Linear: 5-23                           [10, 512, 768]            590,592\n",
      "│    │    │    │    └─RelativePositionEmbedding: 5-24        [10, 512, 768]            --\n",
      "│    │    │    │    │    └─Embedding: 6-13                   [512, 768]                393,216\n",
      "│    │    │    │    │    └─Linear: 6-14                      [512, 768]                590,592\n",
      "│    │    │    │    │    └─Linear: 6-15                      [512, 768]                590,592\n",
      "│    │    │    │    └─AttentionFeedForward: 5-25             [10, 512, 768]            --\n",
      "│    │    │    │    │    └─Linear: 6-16                      [10, 512, 768]            590,592\n",
      "│    │    │    │    │    └─Dropout: 6-17                     [10, 512, 768]            --\n",
      "│    │    │    │    │    └─LayerNorm: 6-18                   [10, 512, 768]            1,536\n",
      "│    │    │    └─TransformerFeedForward: 4-6                 [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-26                           [10, 512, 3072]           2,362,368\n",
      "│    │    │    │    └─GELU: 5-27                             [10, 512, 3072]           --\n",
      "│    │    │    │    └─Linear: 5-28                           [10, 512, 768]            2,360,064\n",
      "│    │    │    │    └─Dropout: 5-29                          [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-30                        [10, 512, 768]            1,536\n",
      "│    │    └─TransformerBlock: 3-4                            [10, 512, 768]            --\n",
      "│    │    │    └─DisentangledSelfAttention: 4-7              [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-31                           [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Linear: 5-32                           [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Linear: 5-33                           [10, 512, 768]            590,592\n",
      "│    │    │    │    └─RelativePositionEmbedding: 5-34        [10, 512, 768]            --\n",
      "│    │    │    │    │    └─Embedding: 6-19                   [512, 768]                393,216\n",
      "│    │    │    │    │    └─Linear: 6-20                      [512, 768]                590,592\n",
      "│    │    │    │    │    └─Linear: 6-21                      [512, 768]                590,592\n",
      "│    │    │    │    └─AttentionFeedForward: 5-35             [10, 512, 768]            --\n",
      "│    │    │    │    │    └─Linear: 6-22                      [10, 512, 768]            590,592\n",
      "│    │    │    │    │    └─Dropout: 6-23                     [10, 512, 768]            --\n",
      "│    │    │    │    │    └─LayerNorm: 6-24                   [10, 512, 768]            1,536\n",
      "│    │    │    └─TransformerFeedForward: 4-8                 [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-36                           [10, 512, 3072]           2,362,368\n",
      "│    │    │    │    └─GELU: 5-37                             [10, 512, 3072]           --\n",
      "│    │    │    │    └─Linear: 5-38                           [10, 512, 768]            2,360,064\n",
      "│    │    │    │    └─Dropout: 5-39                          [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-40                        [10, 512, 768]            1,536\n",
      "│    │    └─TransformerBlock: 3-5                            [10, 512, 768]            --\n",
      "│    │    │    └─DisentangledSelfAttention: 4-9              [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-41                           [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Linear: 5-42                           [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Linear: 5-43                           [10, 512, 768]            590,592\n",
      "│    │    │    │    └─RelativePositionEmbedding: 5-44        [10, 512, 768]            --\n",
      "│    │    │    │    │    └─Embedding: 6-25                   [512, 768]                393,216\n",
      "│    │    │    │    │    └─Linear: 6-26                      [512, 768]                590,592\n",
      "│    │    │    │    │    └─Linear: 6-27                      [512, 768]                590,592\n",
      "│    │    │    │    └─AttentionFeedForward: 5-45             [10, 512, 768]            --\n",
      "│    │    │    │    │    └─Linear: 6-28                      [10, 512, 768]            590,592\n",
      "│    │    │    │    │    └─Dropout: 6-29                     [10, 512, 768]            --\n",
      "│    │    │    │    │    └─LayerNorm: 6-30                   [10, 512, 768]            1,536\n",
      "│    │    │    └─TransformerFeedForward: 4-10                [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-46                           [10, 512, 3072]           2,362,368\n",
      "│    │    │    │    └─GELU: 5-47                             [10, 512, 3072]           --\n",
      "│    │    │    │    └─Linear: 5-48                           [10, 512, 768]            2,360,064\n",
      "│    │    │    │    └─Dropout: 5-49                          [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-50                        [10, 512, 768]            1,536\n",
      "│    │    └─TransformerBlock: 3-6                            [10, 512, 768]            --\n",
      "│    │    │    └─DisentangledSelfAttention: 4-11             [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-51                           [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Linear: 5-52                           [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Linear: 5-53                           [10, 512, 768]            590,592\n",
      "│    │    │    │    └─RelativePositionEmbedding: 5-54        [10, 512, 768]            --\n",
      "│    │    │    │    │    └─Embedding: 6-31                   [512, 768]                393,216\n",
      "│    │    │    │    │    └─Linear: 6-32                      [512, 768]                590,592\n",
      "│    │    │    │    │    └─Linear: 6-33                      [512, 768]                590,592\n",
      "│    │    │    │    └─AttentionFeedForward: 5-55             [10, 512, 768]            --\n",
      "│    │    │    │    │    └─Linear: 6-34                      [10, 512, 768]            590,592\n",
      "│    │    │    │    │    └─Dropout: 6-35                     [10, 512, 768]            --\n",
      "│    │    │    │    │    └─LayerNorm: 6-36                   [10, 512, 768]            1,536\n",
      "│    │    │    └─TransformerFeedForward: 4-12                [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-56                           [10, 512, 3072]           2,362,368\n",
      "│    │    │    │    └─GELU: 5-57                             [10, 512, 3072]           --\n",
      "│    │    │    │    └─Linear: 5-58                           [10, 512, 768]            2,360,064\n",
      "│    │    │    │    └─Dropout: 5-59                          [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-60                        [10, 512, 768]            1,536\n",
      "│    │    └─TransformerBlock: 3-7                            [10, 512, 768]            --\n",
      "│    │    │    └─DisentangledSelfAttention: 4-13             [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-61                           [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Linear: 5-62                           [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Linear: 5-63                           [10, 512, 768]            590,592\n",
      "│    │    │    │    └─RelativePositionEmbedding: 5-64        [10, 512, 768]            --\n",
      "│    │    │    │    │    └─Embedding: 6-37                   [512, 768]                393,216\n",
      "│    │    │    │    │    └─Linear: 6-38                      [512, 768]                590,592\n",
      "│    │    │    │    │    └─Linear: 6-39                      [512, 768]                590,592\n",
      "│    │    │    │    └─AttentionFeedForward: 5-65             [10, 512, 768]            --\n",
      "│    │    │    │    │    └─Linear: 6-40                      [10, 512, 768]            590,592\n",
      "│    │    │    │    │    └─Dropout: 6-41                     [10, 512, 768]            --\n",
      "│    │    │    │    │    └─LayerNorm: 6-42                   [10, 512, 768]            1,536\n",
      "│    │    │    └─TransformerFeedForward: 4-14                [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-66                           [10, 512, 3072]           2,362,368\n",
      "│    │    │    │    └─GELU: 5-67                             [10, 512, 3072]           --\n",
      "│    │    │    │    └─Linear: 5-68                           [10, 512, 768]            2,360,064\n",
      "│    │    │    │    └─Dropout: 5-69                          [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-70                        [10, 512, 768]            1,536\n",
      "│    │    └─TransformerBlock: 3-8                            [10, 512, 768]            --\n",
      "│    │    │    └─DisentangledSelfAttention: 4-15             [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-71                           [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Linear: 5-72                           [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Linear: 5-73                           [10, 512, 768]            590,592\n",
      "│    │    │    │    └─RelativePositionEmbedding: 5-74        [10, 512, 768]            --\n",
      "│    │    │    │    │    └─Embedding: 6-43                   [512, 768]                393,216\n",
      "│    │    │    │    │    └─Linear: 6-44                      [512, 768]                590,592\n",
      "│    │    │    │    │    └─Linear: 6-45                      [512, 768]                590,592\n",
      "│    │    │    │    └─AttentionFeedForward: 5-75             [10, 512, 768]            --\n",
      "│    │    │    │    │    └─Linear: 6-46                      [10, 512, 768]            590,592\n",
      "│    │    │    │    │    └─Dropout: 6-47                     [10, 512, 768]            --\n",
      "│    │    │    │    │    └─LayerNorm: 6-48                   [10, 512, 768]            1,536\n",
      "│    │    │    └─TransformerFeedForward: 4-16                [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-76                           [10, 512, 3072]           2,362,368\n",
      "│    │    │    │    └─GELU: 5-77                             [10, 512, 3072]           --\n",
      "│    │    │    │    └─Linear: 5-78                           [10, 512, 768]            2,360,064\n",
      "│    │    │    │    └─Dropout: 5-79                          [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-80                        [10, 512, 768]            1,536\n",
      "│    │    └─TransformerBlock: 3-9                            [10, 512, 768]            --\n",
      "│    │    │    └─DisentangledSelfAttention: 4-17             [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-81                           [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Linear: 5-82                           [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Linear: 5-83                           [10, 512, 768]            590,592\n",
      "│    │    │    │    └─RelativePositionEmbedding: 5-84        [10, 512, 768]            --\n",
      "│    │    │    │    │    └─Embedding: 6-49                   [512, 768]                393,216\n",
      "│    │    │    │    │    └─Linear: 6-50                      [512, 768]                590,592\n",
      "│    │    │    │    │    └─Linear: 6-51                      [512, 768]                590,592\n",
      "│    │    │    │    └─AttentionFeedForward: 5-85             [10, 512, 768]            --\n",
      "│    │    │    │    │    └─Linear: 6-52                      [10, 512, 768]            590,592\n",
      "│    │    │    │    │    └─Dropout: 6-53                     [10, 512, 768]            --\n",
      "│    │    │    │    │    └─LayerNorm: 6-54                   [10, 512, 768]            1,536\n",
      "│    │    │    └─TransformerFeedForward: 4-18                [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-86                           [10, 512, 3072]           2,362,368\n",
      "│    │    │    │    └─GELU: 5-87                             [10, 512, 3072]           --\n",
      "│    │    │    │    └─Linear: 5-88                           [10, 512, 768]            2,360,064\n",
      "│    │    │    │    └─Dropout: 5-89                          [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-90                        [10, 512, 768]            1,536\n",
      "│    │    └─TransformerBlock: 3-10                           [10, 512, 768]            --\n",
      "│    │    │    └─DisentangledSelfAttention: 4-19             [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-91                           [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Linear: 5-92                           [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Linear: 5-93                           [10, 512, 768]            590,592\n",
      "│    │    │    │    └─RelativePositionEmbedding: 5-94        [10, 512, 768]            --\n",
      "│    │    │    │    │    └─Embedding: 6-55                   [512, 768]                393,216\n",
      "│    │    │    │    │    └─Linear: 6-56                      [512, 768]                590,592\n",
      "│    │    │    │    │    └─Linear: 6-57                      [512, 768]                590,592\n",
      "│    │    │    │    └─AttentionFeedForward: 5-95             [10, 512, 768]            --\n",
      "│    │    │    │    │    └─Linear: 6-58                      [10, 512, 768]            590,592\n",
      "│    │    │    │    │    └─Dropout: 6-59                     [10, 512, 768]            --\n",
      "│    │    │    │    │    └─LayerNorm: 6-60                   [10, 512, 768]            1,536\n",
      "│    │    │    └─TransformerFeedForward: 4-20                [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-96                           [10, 512, 3072]           2,362,368\n",
      "│    │    │    │    └─GELU: 5-97                             [10, 512, 3072]           --\n",
      "│    │    │    │    └─Linear: 5-98                           [10, 512, 768]            2,360,064\n",
      "│    │    │    │    └─Dropout: 5-99                          [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-100                       [10, 512, 768]            1,536\n",
      "│    │    └─TransformerBlock: 3-11                           [10, 512, 768]            --\n",
      "│    │    │    └─DisentangledSelfAttention: 4-21             [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-101                          [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Linear: 5-102                          [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Linear: 5-103                          [10, 512, 768]            590,592\n",
      "│    │    │    │    └─RelativePositionEmbedding: 5-104       [10, 512, 768]            --\n",
      "│    │    │    │    │    └─Embedding: 6-61                   [512, 768]                393,216\n",
      "│    │    │    │    │    └─Linear: 6-62                      [512, 768]                590,592\n",
      "│    │    │    │    │    └─Linear: 6-63                      [512, 768]                590,592\n",
      "│    │    │    │    └─AttentionFeedForward: 5-105            [10, 512, 768]            --\n",
      "│    │    │    │    │    └─Linear: 6-64                      [10, 512, 768]            590,592\n",
      "│    │    │    │    │    └─Dropout: 6-65                     [10, 512, 768]            --\n",
      "│    │    │    │    │    └─LayerNorm: 6-66                   [10, 512, 768]            1,536\n",
      "│    │    │    └─TransformerFeedForward: 4-22                [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-106                          [10, 512, 3072]           2,362,368\n",
      "│    │    │    │    └─GELU: 5-107                            [10, 512, 3072]           --\n",
      "│    │    │    │    └─Linear: 5-108                          [10, 512, 768]            2,360,064\n",
      "│    │    │    │    └─Dropout: 5-109                         [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-110                       [10, 512, 768]            1,536\n",
      "│    │    └─TransformerBlock: 3-12                           [10, 512, 768]            --\n",
      "│    │    │    └─DisentangledSelfAttention: 4-23             [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-111                          [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Linear: 5-112                          [10, 512, 768]            590,592\n",
      "│    │    │    │    └─Linear: 5-113                          [10, 512, 768]            590,592\n",
      "│    │    │    │    └─RelativePositionEmbedding: 5-114       [10, 512, 768]            --\n",
      "│    │    │    │    │    └─Embedding: 6-67                   [512, 768]                393,216\n",
      "│    │    │    │    │    └─Linear: 6-68                      [512, 768]                590,592\n",
      "│    │    │    │    │    └─Linear: 6-69                      [512, 768]                590,592\n",
      "│    │    │    │    └─AttentionFeedForward: 5-115            [10, 512, 768]            --\n",
      "│    │    │    │    │    └─Linear: 6-70                      [10, 512, 768]            590,592\n",
      "│    │    │    │    │    └─Dropout: 6-71                     [10, 512, 768]            --\n",
      "│    │    │    │    │    └─LayerNorm: 6-72                   [10, 512, 768]            1,536\n",
      "│    │    │    └─TransformerFeedForward: 4-24                [10, 512, 768]            --\n",
      "│    │    │    │    └─Linear: 5-116                          [10, 512, 3072]           2,362,368\n",
      "│    │    │    │    └─GELU: 5-117                            [10, 512, 3072]           --\n",
      "│    │    │    │    └─Linear: 5-118                          [10, 512, 768]            2,360,064\n",
      "│    │    │    │    └─Dropout: 5-119                         [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-120                       [10, 512, 768]            1,536\n",
      "├─EnhancedMaskDecoder: 1-3                                   [10, 512, 768]            --\n",
      "├─BaseNetwork: 1-4                                           --                        (recursive)\n",
      "│    └─ModuleList: 2-7                                       --                        (recursive)\n",
      "│    │    └─TransformerBlock: 3-13                           [10, 512, 768]            (recursive)\n",
      "│    │    │    └─DisentangledSelfAttention: 4-25             [10, 512, 768]            (recursive)\n",
      "│    │    │    │    └─Linear: 5-121                          [10, 512, 768]            (recursive)\n",
      "│    │    │    │    └─Linear: 5-122                          [10, 512, 768]            (recursive)\n",
      "│    │    │    │    └─Linear: 5-123                          [10, 512, 768]            (recursive)\n",
      "│    │    │    │    └─RelativePositionEmbedding: 5-124       [10, 512, 768]            (recursive)\n",
      "│    │    │    │    │    └─Embedding: 6-73                   [512, 768]                (recursive)\n",
      "│    │    │    │    │    └─Linear: 6-74                      [512, 768]                (recursive)\n",
      "│    │    │    │    │    └─Linear: 6-75                      [512, 768]                (recursive)\n",
      "│    │    │    │    └─AttentionFeedForward: 5-125            [10, 512, 768]            (recursive)\n",
      "│    │    │    │    │    └─Linear: 6-76                      [10, 512, 768]            (recursive)\n",
      "│    │    │    │    │    └─Dropout: 6-77                     [10, 512, 768]            --\n",
      "│    │    │    │    │    └─LayerNorm: 6-78                   [10, 512, 768]            (recursive)\n",
      "│    │    │    └─TransformerFeedForward: 4-26                [10, 512, 768]            (recursive)\n",
      "│    │    │    │    └─Linear: 5-126                          [10, 512, 3072]           (recursive)\n",
      "│    │    │    │    └─GELU: 5-127                            [10, 512, 3072]           --\n",
      "│    │    │    │    └─Linear: 5-128                          [10, 512, 768]            (recursive)\n",
      "│    │    │    │    └─Dropout: 5-129                         [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-130                       [10, 512, 768]            (recursive)\n",
      "│    │    └─TransformerBlock: 3-14                           [10, 512, 768]            (recursive)\n",
      "│    │    │    └─DisentangledSelfAttention: 4-27             [10, 512, 768]            (recursive)\n",
      "│    │    │    │    └─Linear: 5-131                          [10, 512, 768]            (recursive)\n",
      "│    │    │    │    └─Linear: 5-132                          [10, 512, 768]            (recursive)\n",
      "│    │    │    │    └─Linear: 5-133                          [10, 512, 768]            (recursive)\n",
      "│    │    │    │    └─RelativePositionEmbedding: 5-134       [10, 512, 768]            (recursive)\n",
      "│    │    │    │    │    └─Embedding: 6-79                   [512, 768]                (recursive)\n",
      "│    │    │    │    │    └─Linear: 6-80                      [512, 768]                (recursive)\n",
      "│    │    │    │    │    └─Linear: 6-81                      [512, 768]                (recursive)\n",
      "│    │    │    │    └─AttentionFeedForward: 5-135            [10, 512, 768]            (recursive)\n",
      "│    │    │    │    │    └─Linear: 6-82                      [10, 512, 768]            (recursive)\n",
      "│    │    │    │    │    └─Dropout: 6-83                     [10, 512, 768]            --\n",
      "│    │    │    │    │    └─LayerNorm: 6-84                   [10, 512, 768]            (recursive)\n",
      "│    │    │    └─TransformerFeedForward: 4-28                [10, 512, 768]            (recursive)\n",
      "│    │    │    │    └─Linear: 5-136                          [10, 512, 3072]           (recursive)\n",
      "│    │    │    │    └─GELU: 5-137                            [10, 512, 3072]           --\n",
      "│    │    │    │    └─Linear: 5-138                          [10, 512, 768]            (recursive)\n",
      "│    │    │    │    └─Dropout: 5-139                         [10, 512, 768]            --\n",
      "│    │    │    │    └─LayerNorm: 5-140                       [10, 512, 768]            (recursive)\n",
      "├─ReplacedTokenDiscriminatorHead: 1-5                        [10, 512, 1]              --\n",
      "│    └─Linear: 2-8                                           [10, 512, 768]            590,592\n",
      "│    └─GELU: 2-9                                             [10, 512, 768]            --\n",
      "│    └─LayerNorm: 2-10                                       [10, 512, 768]            1,536\n",
      "│    └─Linear: 2-11                                          [10, 512, 1]              769\n",
      "==============================================================================================================\n",
      "Total params: 137,893,377\n",
      "Trainable params: 137,893,377\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 12.62\n",
      "==============================================================================================================\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 5217.76\n",
      "Params size (MB): 551.57\n",
      "Estimated Total Size (MB): 5769.37\n",
      "==============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "output = test_layer(discriminator, input_data=inputs)\n",
    "summary_layer(discriminator, input_data=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tests.test import test_layer, summary_layer\n",
    "from deberta.config import Config\n",
    "\n",
    "config = Config(\n",
    "    hidden_dim=768,\n",
    "    embedding_dim=1024,\n",
    "    max_seq_len=512,\n",
    "    padding_idx=0,\n",
    "    vocab_size=128001,\n",
    "    absolute_position_biased_input=True,\n",
    "    num_heads=12,\n",
    "    num_head_dim=64,\n",
    "    layernorm_eps=1e-9,\n",
    "    hidden_dropout_prob=0.1,\n",
    "    num_hidden_layers=12,\n",
    "    device='cuda',\n",
    "    mask_lm_prob=0.15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maengmaengeeee/anaconda3/envs/edison/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/maengmaengeeee/anaconda3/envs/edison/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# token masking test\n",
    "from deberta.data import ReplaceTaskData\n",
    "from transformers import AutoTokenizer\n",
    "from pprint import pprint\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')\n",
    "replace_task_data = ReplaceTaskData(config, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
      " 'input_ids': tensor([[     1,    279,   1538,   3258, 128000,  14929,    360,    262, 128000,\n",
      "           1560,      2]]),\n",
      " 'labels': tensor([[    0,     0,     0,     0, 16123,     0,     0,     0,  9118,     0,\n",
      "             0]]),\n",
      " 'original_input_ids': tensor([[    1,   279,  1538,  3258, 16123, 14929,   360,   262,  9118,  1560,\n",
      "             2]])}\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "masked_data = replace_task_data.get_generator_inputs(sentence)\n",
    "pprint(masked_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deberta.networks import Generator, Discriminator\n",
    "\n",
    "generator = Generator(config)\n",
    "discriminator = Discriminator(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = generator(\n",
    "    input_ids=masked_data['input_ids'],\n",
    "    attention_mask=masked_data['attention_mask'],\n",
    "    labels=masked_data['labels'],\n",
    "    labels_mask=masked_data['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11, 128001])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_data = replace_task_data.get_discriminator_inputs(masked_data, logits[0].squeeze(), True)\n",
    "masked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maengmaengeeee/anaconda3/envs/edison/lib/python3.12/site-packages/datasets/load.py:1461: FutureWarning: The repository for wikipedia contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikipedia\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from deberta.fetch_dataset import fetch_dataset\n",
    "\n",
    "dataset = fetch_dataset('wikipedia')\n",
    "dataset = dataset['train']\n",
    "book_dataset = fetch_dataset('bookcorpus')\n",
    "book_dataset = book_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'url': Value(dtype='string', id=None),\n",
       " 'title': Value(dtype='string', id=None),\n",
       " 'text': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch processing\n",
    "The map() function supports working with batches of examples. Operate on batches by setting batched=True. The default batch size is 1000, but you can adjust it with the batch_size parameter. Batch processing enables interesting applications such as splitting long sentences into shorter chunks and data augmentation.\n",
    "\n",
    "### Split long examples\n",
    "When examples are too long, you may want to split them into several smaller chunks. Begin by creating a function that:\n",
    "Splits the sentence1 field into chunks of 50 characters.\n",
    "Stacks all the chunks together to create the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 6458670/6458670 [00:51<00:00, 126149.32 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 74004228/74004228 [00:17<00:00, 4337859.61 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from deberta.prep_dataset import split_sentences\n",
    "\n",
    "dataset = dataset.map(split_sentences, batched=True, num_proc=4, remove_columns=dataset.column_names)\n",
    "book_dataset = book_dataset.map(split_sentences, batched=True, num_proc=4, remove_columns=book_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text'],\n",
       "     num_rows: 28795678\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text'],\n",
       "     num_rows: 74009525\n",
       " }))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset, book_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maengmaengeeee/anaconda3/envs/edison/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map (num_proc=12): 100%|██████████| 28795678/28795678 [22:47<00:00, 21063.15 examples/s]\n",
      "Map (num_proc=12):   7%|▋         | 4873000/74009525 [00:42<08:16, 139125.34 examples/s]"
     ]
    }
   ],
   "source": [
    "from deberta.prep_dataset import SPLIT_IDX, MAX_SEQ_LEN, TOKENIZER\n",
    "\n",
    "TOKENIZER = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')\n",
    "MAX_SEQ_LEN = 512\n",
    "def tokenize_fn(sentences):\n",
    "    tokenized = TOKENIZER(sentences['text'], max_length=MAX_SEQ_LEN, truncation=True, padding=True, return_tensors='pt')\n",
    "    return tokenized\n",
    "\n",
    "dataset = dataset.map(tokenize_fn, batched=True, num_proc=12, remove_columns=dataset.column_names)\n",
    "book_dataset = book_dataset.map(tokenize_fn, batched=True, num_proc=12, remove_columns=book_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edison",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
