{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maengmaengeeee/anaconda3/envs/edison/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/maengmaengeeee/anaconda3/envs/edison/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "base_model = AutoModel.from_pretrained('microsoft/deberta-v3-xsmall')\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-xsmall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "disc_state_dict = torch.load('weights/disc_xsmall.bin')\n",
    "gen_state_dict = torch.load('weights/gen_xsmall.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deberta.embeddings.word_embeddings._weight: torch.Size([128100, 384])\n",
      "deberta.embeddings.word_embeddings.weight: torch.Size([128100, 384])\n",
      "deberta.embeddings.position_embeddings._weight: torch.Size([512, 384])\n",
      "deberta.embeddings.position_embeddings.weight: torch.Size([512, 384])\n",
      "deberta.embeddings.LayerNorm.weight: torch.Size([384])\n",
      "deberta.embeddings.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.0.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.0.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.0.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.0.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.0.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.0.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.1.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.1.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.1.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.1.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.1.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.1.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.2.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.2.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.2.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.2.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.2.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.2.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.3.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.3.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.3.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.3.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.3.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.3.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.4.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.4.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.4.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.4.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.4.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.4.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.5.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.5.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.5.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.5.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.5.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.5.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.6.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.6.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.6.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.6.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.6.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.6.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.7.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.7.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.7.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.7.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.7.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.7.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.8.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.8.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.8.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.8.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.8.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.8.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.9.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.9.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.9.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.9.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.9.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.9.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.10.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.10.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.10.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.10.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.10.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.10.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.11.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.11.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.11.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.11.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.11.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.11.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.rel_embeddings.weight: torch.Size([512, 384])\n",
      "deberta.encoder.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.LayerNorm.bias: torch.Size([384])\n",
      "lm_predictions.lm_head.bias: torch.Size([128100])\n",
      "lm_predictions.lm_head.dense.weight: torch.Size([384, 384])\n",
      "lm_predictions.lm_head.dense.bias: torch.Size([384])\n",
      "lm_predictions.lm_head.LayerNorm.weight: torch.Size([384])\n",
      "lm_predictions.lm_head.LayerNorm.bias: torch.Size([384])\n",
      "mask_predictions.dense.weight: torch.Size([384, 384])\n",
      "mask_predictions.dense.bias: torch.Size([384])\n",
      "mask_predictions.LayerNorm.weight: torch.Size([384])\n",
      "mask_predictions.LayerNorm.bias: torch.Size([384])\n",
      "mask_predictions.classifier.weight: torch.Size([1, 384])\n",
      "mask_predictions.classifier.bias: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for key, value in disc_state_dict.items():\n",
    "    print(f\"{key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deberta.embeddings.word_embeddings.weight: torch.Size([128100, 384])\n",
      "deberta.embeddings.position_embeddings.weight: torch.Size([512, 384])\n",
      "deberta.embeddings.LayerNorm.weight: torch.Size([384])\n",
      "deberta.embeddings.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.0.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.0.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.0.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.0.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.0.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.0.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.1.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.1.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.1.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.1.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.1.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.1.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.2.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.2.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.2.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.2.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.2.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.2.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.3.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.3.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.3.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.3.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.3.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.3.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.4.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.4.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.4.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.4.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.4.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.4.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.5.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.5.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.5.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.5.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.5.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.5.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.rel_embeddings.weight: torch.Size([512, 384])\n",
      "deberta.encoder.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.LayerNorm.bias: torch.Size([384])\n",
      "lm_predictions.lm_head.bias: torch.Size([128100])\n",
      "lm_predictions.lm_head.dense.weight: torch.Size([384, 384])\n",
      "lm_predictions.lm_head.dense.bias: torch.Size([384])\n",
      "lm_predictions.lm_head.LayerNorm.weight: torch.Size([384])\n",
      "lm_predictions.lm_head.LayerNorm.bias: torch.Size([384])\n"
     ]
    }
   ],
   "source": [
    "for key, value in gen_state_dict.items():\n",
    "    print(f\"{key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight: torch.Size([128100, 384])\n",
      "embeddings.LayerNorm.weight: torch.Size([384])\n",
      "embeddings.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.0.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.0.attention.self.query_proj.bias: torch.Size([384])\n",
      "encoder.layer.0.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.0.attention.self.key_proj.bias: torch.Size([384])\n",
      "encoder.layer.0.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.0.attention.self.value_proj.bias: torch.Size([384])\n",
      "encoder.layer.0.attention.output.dense.weight: torch.Size([384, 384])\n",
      "encoder.layer.0.attention.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.0.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.0.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.0.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "encoder.layer.0.intermediate.dense.bias: torch.Size([1536])\n",
      "encoder.layer.0.output.dense.weight: torch.Size([384, 1536])\n",
      "encoder.layer.0.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.0.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.0.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.1.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.1.attention.self.query_proj.bias: torch.Size([384])\n",
      "encoder.layer.1.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.1.attention.self.key_proj.bias: torch.Size([384])\n",
      "encoder.layer.1.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.1.attention.self.value_proj.bias: torch.Size([384])\n",
      "encoder.layer.1.attention.output.dense.weight: torch.Size([384, 384])\n",
      "encoder.layer.1.attention.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.1.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.1.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.1.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "encoder.layer.1.intermediate.dense.bias: torch.Size([1536])\n",
      "encoder.layer.1.output.dense.weight: torch.Size([384, 1536])\n",
      "encoder.layer.1.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.1.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.1.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.2.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.2.attention.self.query_proj.bias: torch.Size([384])\n",
      "encoder.layer.2.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.2.attention.self.key_proj.bias: torch.Size([384])\n",
      "encoder.layer.2.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.2.attention.self.value_proj.bias: torch.Size([384])\n",
      "encoder.layer.2.attention.output.dense.weight: torch.Size([384, 384])\n",
      "encoder.layer.2.attention.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.2.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.2.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.2.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "encoder.layer.2.intermediate.dense.bias: torch.Size([1536])\n",
      "encoder.layer.2.output.dense.weight: torch.Size([384, 1536])\n",
      "encoder.layer.2.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.2.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.2.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.3.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.3.attention.self.query_proj.bias: torch.Size([384])\n",
      "encoder.layer.3.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.3.attention.self.key_proj.bias: torch.Size([384])\n",
      "encoder.layer.3.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.3.attention.self.value_proj.bias: torch.Size([384])\n",
      "encoder.layer.3.attention.output.dense.weight: torch.Size([384, 384])\n",
      "encoder.layer.3.attention.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.3.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.3.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.3.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "encoder.layer.3.intermediate.dense.bias: torch.Size([1536])\n",
      "encoder.layer.3.output.dense.weight: torch.Size([384, 1536])\n",
      "encoder.layer.3.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.3.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.3.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.4.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.4.attention.self.query_proj.bias: torch.Size([384])\n",
      "encoder.layer.4.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.4.attention.self.key_proj.bias: torch.Size([384])\n",
      "encoder.layer.4.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.4.attention.self.value_proj.bias: torch.Size([384])\n",
      "encoder.layer.4.attention.output.dense.weight: torch.Size([384, 384])\n",
      "encoder.layer.4.attention.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.4.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.4.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.4.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "encoder.layer.4.intermediate.dense.bias: torch.Size([1536])\n",
      "encoder.layer.4.output.dense.weight: torch.Size([384, 1536])\n",
      "encoder.layer.4.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.4.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.4.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.5.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.5.attention.self.query_proj.bias: torch.Size([384])\n",
      "encoder.layer.5.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.5.attention.self.key_proj.bias: torch.Size([384])\n",
      "encoder.layer.5.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.5.attention.self.value_proj.bias: torch.Size([384])\n",
      "encoder.layer.5.attention.output.dense.weight: torch.Size([384, 384])\n",
      "encoder.layer.5.attention.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.5.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.5.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.5.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "encoder.layer.5.intermediate.dense.bias: torch.Size([1536])\n",
      "encoder.layer.5.output.dense.weight: torch.Size([384, 1536])\n",
      "encoder.layer.5.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.5.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.5.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.6.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.6.attention.self.query_proj.bias: torch.Size([384])\n",
      "encoder.layer.6.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.6.attention.self.key_proj.bias: torch.Size([384])\n",
      "encoder.layer.6.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.6.attention.self.value_proj.bias: torch.Size([384])\n",
      "encoder.layer.6.attention.output.dense.weight: torch.Size([384, 384])\n",
      "encoder.layer.6.attention.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.6.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.6.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.6.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "encoder.layer.6.intermediate.dense.bias: torch.Size([1536])\n",
      "encoder.layer.6.output.dense.weight: torch.Size([384, 1536])\n",
      "encoder.layer.6.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.6.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.6.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.7.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.7.attention.self.query_proj.bias: torch.Size([384])\n",
      "encoder.layer.7.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.7.attention.self.key_proj.bias: torch.Size([384])\n",
      "encoder.layer.7.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.7.attention.self.value_proj.bias: torch.Size([384])\n",
      "encoder.layer.7.attention.output.dense.weight: torch.Size([384, 384])\n",
      "encoder.layer.7.attention.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.7.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.7.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.7.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "encoder.layer.7.intermediate.dense.bias: torch.Size([1536])\n",
      "encoder.layer.7.output.dense.weight: torch.Size([384, 1536])\n",
      "encoder.layer.7.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.7.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.7.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.8.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.8.attention.self.query_proj.bias: torch.Size([384])\n",
      "encoder.layer.8.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.8.attention.self.key_proj.bias: torch.Size([384])\n",
      "encoder.layer.8.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.8.attention.self.value_proj.bias: torch.Size([384])\n",
      "encoder.layer.8.attention.output.dense.weight: torch.Size([384, 384])\n",
      "encoder.layer.8.attention.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.8.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.8.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.8.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "encoder.layer.8.intermediate.dense.bias: torch.Size([1536])\n",
      "encoder.layer.8.output.dense.weight: torch.Size([384, 1536])\n",
      "encoder.layer.8.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.8.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.8.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.9.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.9.attention.self.query_proj.bias: torch.Size([384])\n",
      "encoder.layer.9.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.9.attention.self.key_proj.bias: torch.Size([384])\n",
      "encoder.layer.9.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.9.attention.self.value_proj.bias: torch.Size([384])\n",
      "encoder.layer.9.attention.output.dense.weight: torch.Size([384, 384])\n",
      "encoder.layer.9.attention.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.9.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.9.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.9.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "encoder.layer.9.intermediate.dense.bias: torch.Size([1536])\n",
      "encoder.layer.9.output.dense.weight: torch.Size([384, 1536])\n",
      "encoder.layer.9.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.9.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.9.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.10.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.10.attention.self.query_proj.bias: torch.Size([384])\n",
      "encoder.layer.10.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.10.attention.self.key_proj.bias: torch.Size([384])\n",
      "encoder.layer.10.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.10.attention.self.value_proj.bias: torch.Size([384])\n",
      "encoder.layer.10.attention.output.dense.weight: torch.Size([384, 384])\n",
      "encoder.layer.10.attention.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.10.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.10.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.10.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "encoder.layer.10.intermediate.dense.bias: torch.Size([1536])\n",
      "encoder.layer.10.output.dense.weight: torch.Size([384, 1536])\n",
      "encoder.layer.10.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.10.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.10.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.11.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.11.attention.self.query_proj.bias: torch.Size([384])\n",
      "encoder.layer.11.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.11.attention.self.key_proj.bias: torch.Size([384])\n",
      "encoder.layer.11.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.11.attention.self.value_proj.bias: torch.Size([384])\n",
      "encoder.layer.11.attention.output.dense.weight: torch.Size([384, 384])\n",
      "encoder.layer.11.attention.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.11.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.11.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.11.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "encoder.layer.11.intermediate.dense.bias: torch.Size([1536])\n",
      "encoder.layer.11.output.dense.weight: torch.Size([384, 1536])\n",
      "encoder.layer.11.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.11.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.11.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.rel_embeddings.weight: torch.Size([512, 384])\n",
      "encoder.LayerNorm.weight: torch.Size([384])\n",
      "encoder.LayerNorm.bias: torch.Size([384])\n"
     ]
    }
   ],
   "source": [
    "for key, value in base_model.named_parameters():\n",
    "    print(f\"{key}: {value.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maengmaengeeee/anaconda3/envs/edison/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from edison.first.module import LM\n",
    "from edison.config.config import Config\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "config = Config(\n",
    "    hidden_dim=384,\n",
    "    embedding_dim=384,\n",
    "    max_seq_len=512,\n",
    "    padding_idx=0,\n",
    "    vocab_size=128001,\n",
    "    absolute_position_biased_input=True,\n",
    "    num_heads=6,\n",
    "    num_head_dim=64,\n",
    "    layernorm_eps=1e-7,\n",
    "    hidden_dropout_prob=0.1,\n",
    "    num_hidden_layers=12,\n",
    "    device='cuda',\n",
    "    mask_lm_prob=0.15,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')\n",
    "model = LM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_map = {\n",
    "    'deberta.embeddings.word_embeddings.weight': 'discriminator.embedding.word_embedding_layer.weight',\n",
    "    'deberta.embeddings.word_embeddings._weight': 'discriminator.embedding.word_embedding_layer._weight',\n",
    "    'deberta.embeddings.position_embeddings.weight': 'discriminator.embedding.absolute_position_embedding_layer.weight',\n",
    "    'deberta.embeddings.position_embeddings._weight': 'discriminator.embedding.absolute_position_embedding_layer._weight',\n",
    "    'deberta.embeddings.LayerNorm.weight': 'discriminator.embedding.layernorm.weight',\n",
    "    'deberta.embeddings.LayerNorm.bias': 'discriminator.embedding.layernorm.bias',\n",
    "\n",
    "    'attention.self.query_proj.weight': 'attention.query_layer.weight',\n",
    "    'attention.self.query_proj.bias': 'attention.query_layer.bias',\n",
    "    'attention.self.key_proj.weight': 'attention.key_layer.weight',\n",
    "    'attention.self.key_proj.bias': 'attention.key_layer.bias',\n",
    "    'attention.self.value_proj.weight': 'attention.value_layer.weight',\n",
    "    'attention.self.value_proj.bias': 'attention.value_layer.bias',\n",
    "    'attention.output.dense.weight': 'attention.feedforward.weight',\n",
    "    'attention.output.dense.bias': 'attention.feedforward.bias',\n",
    "    'attention.output.LayerNorm.weight': 'attention.feedforward.layernorm.weight',\n",
    "    'attention.output.LayerNorm.bias': 'attention.feedforward.layernorm.bias',\n",
    "    'intermediate.dense.weight': 'feedforward.feedforward_1.weight',\n",
    "    'intermediate.dense.bias': 'feedforward.feedforward_1.bias',\n",
    "    'output.dense.weight': 'feedforward.feedforward_2.weight',\n",
    "    'output.dense.bias': 'feedforward.feedforward_2.bias',\n",
    "    'output.LayerNorm.weight': 'feedforward.layernorm.weight',\n",
    "    'output.LayerNorm.bias': 'feedforward.layernorm.bias',\n",
    "    \n",
    "    'deberta.encoder.rel_embeddings.weight': 'discriminator.relative_position_embedding.relative_position_embedding_layer.weight',\n",
    "    'deberta.encoder.LayerNorm.weight': 'discriminator.relative_position_embedding.layernorm.weight',\n",
    "    'deberta.encoder.LayerNorm.bias': 'discriminator.relative_position_embedding.layernorm.bias',\n",
    "    'mask_predictions.dense.weight': 'discriminator.head.dense.weight',\n",
    "    'mask_predictions.dense.bias': 'discriminator.head.dense.bias',\n",
    "    'mask_predictions.LayerNorm.weight': 'discriminator.head.layernorm.weight',\n",
    "    'mask_predictions.LayerNorm.bias': 'discriminator.head.layernorm.bias',\n",
    "    'mask_predictions.classifier.weight': 'discriminator.head.classifier.weight',\n",
    "    'mask_predictions.classifier.bias': 'discriminator.head.classifier.bias',\n",
    "    'lm_predictions.lm_head.bias': 'generator.head.bias',\n",
    "    'lm_predictions.lm_head.dense.weight': 'generator.head.dense.weight',\n",
    "    'lm_predictions.lm_head.dense.bias': 'generator.head.dense.bias',\n",
    "    'lm_predictions.lm_head.LayerNorm.weight': 'generator.head.layernorm.weight',\n",
    "    'lm_predictions.lm_head.LayerNorm.bias': 'generator.head.layernorm.bias',\n",
    "}\n",
    "weight_path = 'weights/disc_xsmall.bin'\n",
    "base_state_dict = torch.load(weight_path)\n",
    "disc_state_dict = {}\n",
    "idx = None\n",
    "for k, v in base_state_dict.items():\n",
    "    if k.startswith('deberta.encoder.layer'):\n",
    "        prefix = 'discriminator.encoder.layers'\n",
    "        idx = int(k.split('.')[3])\n",
    "        k = '.'.join(k.split('.')[4:])\n",
    "    if k in layer_map:\n",
    "        k = layer_map[k]\n",
    "    try:\n",
    "        key = f'{prefix}.{idx}.{k}' if idx is not None else k\n",
    "        disc_state_dict[key] = v\n",
    "        idx = None\n",
    "    except KeyError:\n",
    "        print(f'KeyError: {k}')\n",
    "# model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator.embedding.word_embedding_layer._weight: torch.Size([128100, 384])\n",
      "discriminator.embedding.word_embedding_layer.weight: torch.Size([128100, 384])\n",
      "discriminator.embedding.absolute_position_embedding_layer._weight: torch.Size([512, 384])\n",
      "discriminator.embedding.absolute_position_embedding_layer.weight: torch.Size([512, 384])\n",
      "discriminator.embedding.layernorm.weight: torch.Size([384])\n",
      "discriminator.embedding.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.0.attention.query_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.0.attention.query_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.0.attention.key_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.0.attention.key_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.0.attention.value_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.0.attention.value_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.0.attention.feedforward.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.0.attention.feedforward.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.0.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.0.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.0.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "discriminator.encoder.layers.0.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "discriminator.encoder.layers.0.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "discriminator.encoder.layers.0.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.0.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.0.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.1.attention.query_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.1.attention.query_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.1.attention.key_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.1.attention.key_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.1.attention.value_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.1.attention.value_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.1.attention.feedforward.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.1.attention.feedforward.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.1.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.1.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.1.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "discriminator.encoder.layers.1.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "discriminator.encoder.layers.1.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "discriminator.encoder.layers.1.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.1.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.1.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.2.attention.query_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.2.attention.query_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.2.attention.key_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.2.attention.key_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.2.attention.value_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.2.attention.value_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.2.attention.feedforward.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.2.attention.feedforward.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.2.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.2.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.2.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "discriminator.encoder.layers.2.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "discriminator.encoder.layers.2.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "discriminator.encoder.layers.2.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.2.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.2.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.3.attention.query_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.3.attention.query_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.3.attention.key_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.3.attention.key_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.3.attention.value_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.3.attention.value_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.3.attention.feedforward.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.3.attention.feedforward.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.3.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.3.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.3.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "discriminator.encoder.layers.3.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "discriminator.encoder.layers.3.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "discriminator.encoder.layers.3.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.3.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.3.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.4.attention.query_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.4.attention.query_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.4.attention.key_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.4.attention.key_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.4.attention.value_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.4.attention.value_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.4.attention.feedforward.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.4.attention.feedforward.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.4.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.4.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.4.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "discriminator.encoder.layers.4.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "discriminator.encoder.layers.4.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "discriminator.encoder.layers.4.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.4.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.4.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.5.attention.query_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.5.attention.query_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.5.attention.key_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.5.attention.key_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.5.attention.value_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.5.attention.value_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.5.attention.feedforward.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.5.attention.feedforward.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.5.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.5.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.5.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "discriminator.encoder.layers.5.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "discriminator.encoder.layers.5.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "discriminator.encoder.layers.5.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.5.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.5.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.6.attention.query_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.6.attention.query_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.6.attention.key_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.6.attention.key_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.6.attention.value_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.6.attention.value_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.6.attention.feedforward.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.6.attention.feedforward.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.6.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.6.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.6.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "discriminator.encoder.layers.6.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "discriminator.encoder.layers.6.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "discriminator.encoder.layers.6.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.6.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.6.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.7.attention.query_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.7.attention.query_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.7.attention.key_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.7.attention.key_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.7.attention.value_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.7.attention.value_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.7.attention.feedforward.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.7.attention.feedforward.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.7.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.7.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.7.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "discriminator.encoder.layers.7.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "discriminator.encoder.layers.7.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "discriminator.encoder.layers.7.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.7.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.7.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.8.attention.query_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.8.attention.query_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.8.attention.key_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.8.attention.key_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.8.attention.value_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.8.attention.value_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.8.attention.feedforward.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.8.attention.feedforward.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.8.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.8.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.8.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "discriminator.encoder.layers.8.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "discriminator.encoder.layers.8.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "discriminator.encoder.layers.8.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.8.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.8.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.9.attention.query_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.9.attention.query_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.9.attention.key_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.9.attention.key_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.9.attention.value_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.9.attention.value_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.9.attention.feedforward.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.9.attention.feedforward.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.9.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.9.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.9.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "discriminator.encoder.layers.9.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "discriminator.encoder.layers.9.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "discriminator.encoder.layers.9.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.9.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.9.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.10.attention.query_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.10.attention.query_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.10.attention.key_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.10.attention.key_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.10.attention.value_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.10.attention.value_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.10.attention.feedforward.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.10.attention.feedforward.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.10.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.10.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.10.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "discriminator.encoder.layers.10.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "discriminator.encoder.layers.10.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "discriminator.encoder.layers.10.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.10.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.10.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.11.attention.query_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.11.attention.query_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.11.attention.key_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.11.attention.key_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.11.attention.value_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.11.attention.value_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.11.attention.feedforward.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.11.attention.feedforward.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.11.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.11.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.11.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "discriminator.encoder.layers.11.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "discriminator.encoder.layers.11.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "discriminator.encoder.layers.11.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.11.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.11.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.relative_position_embedding.relative_position_embedding_layer.weight: torch.Size([512, 384])\n",
      "discriminator.relative_position_embedding.layernorm.weight: torch.Size([384])\n",
      "discriminator.relative_position_embedding.layernorm.bias: torch.Size([384])\n",
      "generator.head.bias: torch.Size([128100])\n",
      "generator.head.dense.weight: torch.Size([384, 384])\n",
      "generator.head.dense.bias: torch.Size([384])\n",
      "generator.head.layernorm.weight: torch.Size([384])\n",
      "generator.head.layernorm.bias: torch.Size([384])\n",
      "discriminator.head.dense.weight: torch.Size([384, 384])\n",
      "discriminator.head.dense.bias: torch.Size([384])\n",
      "discriminator.head.layernorm.weight: torch.Size([384])\n",
      "discriminator.head.layernorm.bias: torch.Size([384])\n",
      "discriminator.head.classifier.weight: torch.Size([1, 384])\n",
      "discriminator.head.classifier.bias: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for key, value in disc_state_dict.items():\n",
    "    print(f\"{key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m weight_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights/gen_xsmall.bin\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m base_state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mload(weight_path)  \u001b[38;5;66;03m# gen\u001b[39;00m\n\u001b[1;32m      3\u001b[0m gen_state_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      4\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "weight_path = 'weights/gen_xsmall.bin'\n",
    "base_state_dict = torch.load(weight_path)  # gen\n",
    "gen_state_dict = {}\n",
    "idx = None\n",
    "for k, v in base_state_dict.items():\n",
    "    if k.startswith('deberta.encoder.layer'):\n",
    "        prefix = 'generator.encoder.layers'\n",
    "        idx = int(k.split('.')[3])\n",
    "        k = '.'.join(k.split('.')[4:])\n",
    "    if k in layer_map:\n",
    "        k = layer_map[k]\n",
    "    try:\n",
    "        key = f'{prefix}.{idx}.{k}' if idx is not None else k\n",
    "        gen_state_dict[key] = v\n",
    "        idx = None\n",
    "    except KeyError:\n",
    "        print(f'KeyError: {k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator.embedding.word_embedding_layer.weight: torch.Size([128100, 384])\n",
      "discriminator.embedding.absolute_position_embedding_layer.weight: torch.Size([512, 384])\n",
      "discriminator.embedding.layernorm.weight: torch.Size([384])\n",
      "discriminator.embedding.layernorm.bias: torch.Size([384])\n",
      "generator.encoder.layers.0.attention.query_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.0.attention.query_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.0.attention.key_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.0.attention.key_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.0.attention.value_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.0.attention.value_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.0.attention.feedforward.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.0.attention.feedforward.bias: torch.Size([384])\n",
      "generator.encoder.layers.0.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "generator.encoder.layers.0.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "generator.encoder.layers.0.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "generator.encoder.layers.0.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "generator.encoder.layers.0.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "generator.encoder.layers.0.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "generator.encoder.layers.0.feedforward.layernorm.weight: torch.Size([384])\n",
      "generator.encoder.layers.0.feedforward.layernorm.bias: torch.Size([384])\n",
      "generator.encoder.layers.1.attention.query_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.1.attention.query_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.1.attention.key_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.1.attention.key_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.1.attention.value_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.1.attention.value_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.1.attention.feedforward.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.1.attention.feedforward.bias: torch.Size([384])\n",
      "generator.encoder.layers.1.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "generator.encoder.layers.1.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "generator.encoder.layers.1.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "generator.encoder.layers.1.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "generator.encoder.layers.1.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "generator.encoder.layers.1.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "generator.encoder.layers.1.feedforward.layernorm.weight: torch.Size([384])\n",
      "generator.encoder.layers.1.feedforward.layernorm.bias: torch.Size([384])\n",
      "generator.encoder.layers.2.attention.query_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.2.attention.query_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.2.attention.key_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.2.attention.key_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.2.attention.value_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.2.attention.value_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.2.attention.feedforward.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.2.attention.feedforward.bias: torch.Size([384])\n",
      "generator.encoder.layers.2.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "generator.encoder.layers.2.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "generator.encoder.layers.2.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "generator.encoder.layers.2.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "generator.encoder.layers.2.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "generator.encoder.layers.2.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "generator.encoder.layers.2.feedforward.layernorm.weight: torch.Size([384])\n",
      "generator.encoder.layers.2.feedforward.layernorm.bias: torch.Size([384])\n",
      "generator.encoder.layers.3.attention.query_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.3.attention.query_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.3.attention.key_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.3.attention.key_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.3.attention.value_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.3.attention.value_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.3.attention.feedforward.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.3.attention.feedforward.bias: torch.Size([384])\n",
      "generator.encoder.layers.3.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "generator.encoder.layers.3.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "generator.encoder.layers.3.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "generator.encoder.layers.3.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "generator.encoder.layers.3.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "generator.encoder.layers.3.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "generator.encoder.layers.3.feedforward.layernorm.weight: torch.Size([384])\n",
      "generator.encoder.layers.3.feedforward.layernorm.bias: torch.Size([384])\n",
      "generator.encoder.layers.4.attention.query_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.4.attention.query_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.4.attention.key_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.4.attention.key_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.4.attention.value_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.4.attention.value_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.4.attention.feedforward.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.4.attention.feedforward.bias: torch.Size([384])\n",
      "generator.encoder.layers.4.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "generator.encoder.layers.4.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "generator.encoder.layers.4.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "generator.encoder.layers.4.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "generator.encoder.layers.4.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "generator.encoder.layers.4.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "generator.encoder.layers.4.feedforward.layernorm.weight: torch.Size([384])\n",
      "generator.encoder.layers.4.feedforward.layernorm.bias: torch.Size([384])\n",
      "generator.encoder.layers.5.attention.query_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.5.attention.query_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.5.attention.key_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.5.attention.key_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.5.attention.value_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.5.attention.value_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.5.attention.feedforward.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.5.attention.feedforward.bias: torch.Size([384])\n",
      "generator.encoder.layers.5.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "generator.encoder.layers.5.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "generator.encoder.layers.5.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "generator.encoder.layers.5.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "generator.encoder.layers.5.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "generator.encoder.layers.5.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "generator.encoder.layers.5.feedforward.layernorm.weight: torch.Size([384])\n",
      "generator.encoder.layers.5.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.relative_position_embedding.relative_position_embedding_layer.weight: torch.Size([512, 384])\n",
      "discriminator.relative_position_embedding.layernorm.weight: torch.Size([384])\n",
      "discriminator.relative_position_embedding.layernorm.bias: torch.Size([384])\n",
      "generator.head.bias: torch.Size([128100])\n",
      "generator.head.dense.weight: torch.Size([384, 384])\n",
      "generator.head.dense.bias: torch.Size([384])\n",
      "generator.head.layernorm.weight: torch.Size([384])\n",
      "generator.head.layernorm.bias: torch.Size([384])\n"
     ]
    }
   ],
   "source": [
    "for key, value in gen_state_dict.items():\n",
    "    print(f\"{key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maengmaengeeee/anaconda3/envs/edison/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not included: deberta.embeddings.word_embeddings.weight\n",
      "not included: deberta.embeddings.position_embeddings.weight\n",
      "not included: lm_predictions.lm_head.bias\n",
      "not included: lm_predictions.lm_head.dense.weight\n",
      "not included: lm_predictions.lm_head.dense.bias\n",
      "not included: lm_predictions.lm_head.LayerNorm.weight\n",
      "not included: lm_predictions.lm_head.LayerNorm.bias\n"
     ]
    }
   ],
   "source": [
    "from edison.first.load_state import load_pretrained_LM\n",
    "\n",
    "model, disc, gen = load_pretrained_LM(\n",
    "    ['weights/disc_xsmall.bin', 'weights/gen_xsmall.bin'],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.word_embedding_layer._weight: torch.Size([128100, 384])\n",
      "embedding.absolute_position_embedding_layer._weight: torch.Size([512, 384])\n",
      "embedding.layernorm.weight: torch.Size([384])\n",
      "embedding.layernorm.bias: torch.Size([384])\n",
      "relative_position_embedding.relative_position_embedding_layer.weight: torch.Size([512, 384])\n",
      "relative_position_embedding.layernorm.weight: torch.Size([384])\n",
      "relative_position_embedding.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.0.attention.query_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.0.attention.query_layer.bias: torch.Size([384])\n",
      "encoder.layers.0.attention.key_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.0.attention.key_layer.bias: torch.Size([384])\n",
      "encoder.layers.0.attention.value_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.0.attention.value_layer.bias: torch.Size([384])\n",
      "encoder.layers.0.attention.feedforward.dense.weight: torch.Size([384, 384])\n",
      "encoder.layers.0.attention.feedforward.dense.bias: torch.Size([384])\n",
      "encoder.layers.0.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.0.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.0.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "encoder.layers.0.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "encoder.layers.0.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "encoder.layers.0.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "encoder.layers.0.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.0.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.1.attention.query_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.1.attention.query_layer.bias: torch.Size([384])\n",
      "encoder.layers.1.attention.key_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.1.attention.key_layer.bias: torch.Size([384])\n",
      "encoder.layers.1.attention.value_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.1.attention.value_layer.bias: torch.Size([384])\n",
      "encoder.layers.1.attention.feedforward.dense.weight: torch.Size([384, 384])\n",
      "encoder.layers.1.attention.feedforward.dense.bias: torch.Size([384])\n",
      "encoder.layers.1.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.1.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.1.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "encoder.layers.1.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "encoder.layers.1.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "encoder.layers.1.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "encoder.layers.1.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.1.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.2.attention.query_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.2.attention.query_layer.bias: torch.Size([384])\n",
      "encoder.layers.2.attention.key_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.2.attention.key_layer.bias: torch.Size([384])\n",
      "encoder.layers.2.attention.value_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.2.attention.value_layer.bias: torch.Size([384])\n",
      "encoder.layers.2.attention.feedforward.dense.weight: torch.Size([384, 384])\n",
      "encoder.layers.2.attention.feedforward.dense.bias: torch.Size([384])\n",
      "encoder.layers.2.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.2.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.2.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "encoder.layers.2.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "encoder.layers.2.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "encoder.layers.2.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "encoder.layers.2.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.2.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.3.attention.query_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.3.attention.query_layer.bias: torch.Size([384])\n",
      "encoder.layers.3.attention.key_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.3.attention.key_layer.bias: torch.Size([384])\n",
      "encoder.layers.3.attention.value_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.3.attention.value_layer.bias: torch.Size([384])\n",
      "encoder.layers.3.attention.feedforward.dense.weight: torch.Size([384, 384])\n",
      "encoder.layers.3.attention.feedforward.dense.bias: torch.Size([384])\n",
      "encoder.layers.3.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.3.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.3.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "encoder.layers.3.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "encoder.layers.3.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "encoder.layers.3.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "encoder.layers.3.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.3.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.4.attention.query_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.4.attention.query_layer.bias: torch.Size([384])\n",
      "encoder.layers.4.attention.key_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.4.attention.key_layer.bias: torch.Size([384])\n",
      "encoder.layers.4.attention.value_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.4.attention.value_layer.bias: torch.Size([384])\n",
      "encoder.layers.4.attention.feedforward.dense.weight: torch.Size([384, 384])\n",
      "encoder.layers.4.attention.feedforward.dense.bias: torch.Size([384])\n",
      "encoder.layers.4.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.4.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.4.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "encoder.layers.4.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "encoder.layers.4.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "encoder.layers.4.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "encoder.layers.4.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.4.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.5.attention.query_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.5.attention.query_layer.bias: torch.Size([384])\n",
      "encoder.layers.5.attention.key_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.5.attention.key_layer.bias: torch.Size([384])\n",
      "encoder.layers.5.attention.value_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.5.attention.value_layer.bias: torch.Size([384])\n",
      "encoder.layers.5.attention.feedforward.dense.weight: torch.Size([384, 384])\n",
      "encoder.layers.5.attention.feedforward.dense.bias: torch.Size([384])\n",
      "encoder.layers.5.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.5.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.5.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "encoder.layers.5.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "encoder.layers.5.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "encoder.layers.5.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "encoder.layers.5.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.5.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.6.attention.query_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.6.attention.query_layer.bias: torch.Size([384])\n",
      "encoder.layers.6.attention.key_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.6.attention.key_layer.bias: torch.Size([384])\n",
      "encoder.layers.6.attention.value_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.6.attention.value_layer.bias: torch.Size([384])\n",
      "encoder.layers.6.attention.feedforward.dense.weight: torch.Size([384, 384])\n",
      "encoder.layers.6.attention.feedforward.dense.bias: torch.Size([384])\n",
      "encoder.layers.6.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.6.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.6.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "encoder.layers.6.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "encoder.layers.6.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "encoder.layers.6.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "encoder.layers.6.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.6.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.7.attention.query_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.7.attention.query_layer.bias: torch.Size([384])\n",
      "encoder.layers.7.attention.key_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.7.attention.key_layer.bias: torch.Size([384])\n",
      "encoder.layers.7.attention.value_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.7.attention.value_layer.bias: torch.Size([384])\n",
      "encoder.layers.7.attention.feedforward.dense.weight: torch.Size([384, 384])\n",
      "encoder.layers.7.attention.feedforward.dense.bias: torch.Size([384])\n",
      "encoder.layers.7.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.7.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.7.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "encoder.layers.7.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "encoder.layers.7.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "encoder.layers.7.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "encoder.layers.7.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.7.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.8.attention.query_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.8.attention.query_layer.bias: torch.Size([384])\n",
      "encoder.layers.8.attention.key_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.8.attention.key_layer.bias: torch.Size([384])\n",
      "encoder.layers.8.attention.value_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.8.attention.value_layer.bias: torch.Size([384])\n",
      "encoder.layers.8.attention.feedforward.dense.weight: torch.Size([384, 384])\n",
      "encoder.layers.8.attention.feedforward.dense.bias: torch.Size([384])\n",
      "encoder.layers.8.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.8.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.8.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "encoder.layers.8.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "encoder.layers.8.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "encoder.layers.8.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "encoder.layers.8.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.8.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.9.attention.query_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.9.attention.query_layer.bias: torch.Size([384])\n",
      "encoder.layers.9.attention.key_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.9.attention.key_layer.bias: torch.Size([384])\n",
      "encoder.layers.9.attention.value_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.9.attention.value_layer.bias: torch.Size([384])\n",
      "encoder.layers.9.attention.feedforward.dense.weight: torch.Size([384, 384])\n",
      "encoder.layers.9.attention.feedforward.dense.bias: torch.Size([384])\n",
      "encoder.layers.9.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.9.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.9.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "encoder.layers.9.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "encoder.layers.9.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "encoder.layers.9.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "encoder.layers.9.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.9.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.10.attention.query_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.10.attention.query_layer.bias: torch.Size([384])\n",
      "encoder.layers.10.attention.key_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.10.attention.key_layer.bias: torch.Size([384])\n",
      "encoder.layers.10.attention.value_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.10.attention.value_layer.bias: torch.Size([384])\n",
      "encoder.layers.10.attention.feedforward.dense.weight: torch.Size([384, 384])\n",
      "encoder.layers.10.attention.feedforward.dense.bias: torch.Size([384])\n",
      "encoder.layers.10.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.10.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.10.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "encoder.layers.10.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "encoder.layers.10.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "encoder.layers.10.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "encoder.layers.10.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.10.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.11.attention.query_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.11.attention.query_layer.bias: torch.Size([384])\n",
      "encoder.layers.11.attention.key_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.11.attention.key_layer.bias: torch.Size([384])\n",
      "encoder.layers.11.attention.value_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.11.attention.value_layer.bias: torch.Size([384])\n",
      "encoder.layers.11.attention.feedforward.dense.weight: torch.Size([384, 384])\n",
      "encoder.layers.11.attention.feedforward.dense.bias: torch.Size([384])\n",
      "encoder.layers.11.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.11.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.11.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "encoder.layers.11.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "encoder.layers.11.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "encoder.layers.11.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "encoder.layers.11.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.11.feedforward.layernorm.bias: torch.Size([384])\n",
      "head.dense.weight: torch.Size([384, 384])\n",
      "head.dense.bias: torch.Size([384])\n",
      "head.layernorm.weight: torch.Size([384])\n",
      "head.layernorm.bias: torch.Size([384])\n",
      "head.classifier.weight: torch.Size([1, 384])\n",
      "head.classifier.bias: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for key, value in model.discriminator.named_parameters():\n",
    "    print(f\"{key}: {value.size()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edison",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
