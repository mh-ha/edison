{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim, utils\n",
    "from torch.nn import functional as F\n",
    "import lightning as L\n",
    "\n",
    "from transformers.models.bart.modeling_bart import BartForConditionalGeneration, shift_tokens_right\n",
    "from transformers import AutoTokenizer, BatchEncoding, PreTrainedTokenizerBase\n",
    "\n",
    "from latent_models.perceiver_ae import PerceiverAutoEncoder\n",
    "\n",
    "class LatentGenerator(L.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            pretrained_model: BartForConditionalGeneration,\n",
    "            autoencoder: PerceiverAutoEncoder,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        self.lm = pretrained_model\n",
    "        self.autoencoder = autoencoder\n",
    "\n",
    "        # freeze pretrained model\n",
    "        self.lm.requires_grad_(False)\n",
    "\n",
    "    # required\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = self.lm.get_encoder()(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "        x = self.autoencoder.encode(x['last_hidden_state'], attention_mask=batch['attention_mask'])\n",
    "        x = self.autoencoder.decode(x)\n",
    "        loss = self.lm(labels=batch['labels'], encoder_outputs=x).loss\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    # required\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=1e-3)\n",
    "    \n",
    "    # optional\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = self.lm.get_encoder()(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "        x = self.autoencoder.encode(x['last_hidden_state'], attention_mask=batch['attention_mask'])\n",
    "        x = self.autoencoder.decode(x)\n",
    "        loss = self.lm(labels=batch['labels'], encoder_outputs=x).loss\n",
    "        self.log('valid_loss', loss)\n",
    "        return loss\n",
    "\n",
    "class Diffusion(L.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            diffusion_model,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        self.diffusion_model = diffusion_model\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForBartDenoisingLM:\n",
    "    \"\"\"\n",
    "    Data collator used for BART denoising language modeling.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (:class:`~transformers.PreTrainedTokenizer` or :class:`~transformers.PreTrainedTokenizerFast`):\n",
    "            The tokenizer used for encoding the data\n",
    "    \"\"\"\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, examples: List[Dict[str, List[int]]]) -> BatchEncoding:\n",
    "        batch = BatchEncoding(\n",
    "            {k: torch.LongTensor([examples[i][k] for i in range(len(examples))]) for k, v in examples[0].items()}\n",
    "        )\n",
    "        batch[\"labels\"] = batch[\"input_ids\"].clone()\n",
    "        batch[\"decoder_input_ids\"] = shift_tokens_right(\n",
    "            batch[\"labels\"], self.tokenizer.pad_token_id, self.decoder_start_token_id\n",
    "        )\n",
    "        batch['labels'][batch['labels'] == self.tokenizer.pad_token_id] = -100\n",
    "        batch[\"attention_mask\"] = (batch[\"input_ids\"] != self.tokenizer.pad_token_id).long()\n",
    "        batch[\"decoder_attention_mask\"] = (batch[\"decoder_input_ids\"] != self.tokenizer.pad_token_id).long()\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 93161/93161 [00:01<00:00, 79028.13 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 78896.95 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 93161/93161 [00:02<00:00, 40896.91 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 8719.82 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 4000/4000 [00:00<00:00, 21270.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "enc_dec_model = 'facebook/bart-base'\n",
    "max_seq_len = 64\n",
    "dataset_name = 'roc'\n",
    "train_batch_size = 32\n",
    "\n",
    "pretrained_model = BartForConditionalGeneration.from_pretrained(enc_dec_model)\n",
    "autoencoder = PerceiverAutoEncoder(\n",
    "    dim_lm=pretrained_model.config.d_model,\n",
    "    dim_ae=64,\n",
    "    depth=3,\n",
    "    num_encoder_latents=32,\n",
    "    num_decoder_latents=32,\n",
    "    max_seq_len=max_seq_len,\n",
    "    transformer_decoder=True,\n",
    "    l2_normalize_latents=False,\n",
    ")\n",
    "latent_generator = LatentGenerator(pretrained_model, autoencoder)\n",
    "tokenizer = AutoTokenizer.from_pretrained(enc_dec_model)\n",
    "model_config = pretrained_model.config\n",
    "\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "roc_data_path = 'datasets/ROCstory'\n",
    "dataset = load_dataset(\"text\", data_files={f'{split}': os.path.join(roc_data_path, f'roc_{split}.json') for split in ['train', 'valid']})\n",
    "def process_roc_dataset(dataset):\n",
    "    def extract_roc_text(example):\n",
    "        text = example['text']\n",
    "        assert text[:2] == '[\"'\n",
    "        assert text[-2:] == '\"]'\n",
    "        sentences = text[2:-2]\n",
    "        return {'text': sentences}\n",
    "    dataset = dataset.map(extract_roc_text, )\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "    # Hold out some validation samples for testing\n",
    "    val_test_ds = dataset['valid'].train_test_split(train_size=1000, shuffle=False)\n",
    "    dataset['valid'] = val_test_ds['train']\n",
    "    dataset['test'] = val_test_ds['test']\n",
    "    return dataset\n",
    "prep_dataset = process_roc_dataset(dataset)\n",
    "map_prep_dataset = prep_dataset.map(lambda x: tokenizer(x['text'], padding=\"max_length\", truncation=True, max_length=max_seq_len), batched=True, remove_columns=['text'], num_proc=4)\n",
    "# map_prep_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "# collate_fn = lambda x: tokenizer.pad(x, return_tensors='pt')\n",
    "collate_fn = DataCollatorForBartDenoisingLM(tokenizer, model_config.decoder_start_token_id)\n",
    "train_dataloader = DataLoader(map_prep_dataset['train'], batch_size=32, shuffle=True, collate_fn=collate_fn, drop_last=True, num_workers=4)\n",
    "valid_dataloader = DataLoader(map_prep_dataset['valid'], batch_size=32, shuffle=False, collate_fn=collate_fn, drop_last=True, num_workers=4)\n",
    "test_dataloader = DataLoader(map_prep_dataset['test'], batch_size=32, shuffle=False, collate_fn=collate_fn, drop_last=True, num_workers=4)\n",
    "# train_dataloader = DataLoader(prep_dataset['train'], batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "# valid_dataloader = DataLoader(prep_dataset['valid'], batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "# test_dataloader = DataLoader(prep_dataset['test'], batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_dataloader:\n",
    "    break\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "i = i.to('cuda')\n",
    "result = summary(latent_generator.lm, input_data={**i}, device='cuda', depth=10)\n",
    "input_ae = latent_generator.lm.get_encoder()(input_ids=i['input_ids'], attention_mask=i['attention_mask'])\n",
    "input_ae_attn = i['attention_mask']\n",
    "result_2 = summary(latent_generator.autoencoder, input_data={'encoder_outputs': input_ae['last_hidden_state'], 'attention_mask': input_ae_attn}, device='cuda', depth=10)\n",
    "# output_ae = latent_generator.autoencoder(input_ae['last_hidden_state'], attention_mask=input_ae_attn)\n",
    "# output_ae_attn = input_ae_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maengmaengeeee/anaconda3/envs/edison/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "latent_model = torch.load('saved_latent_models/roc/2024-03-24_17-58-54/model_full.pt')\n",
    "diffusion_model = torch.load('saved_diff_models/roc/2024-03-24_17-59-30/diffusion.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'latent_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output_ae \u001b[38;5;241m=\u001b[39m \u001b[43mlatent_generator\u001b[49m\u001b[38;5;241m.\u001b[39mautoencoder\u001b[38;5;241m.\u001b[39mencode(input_ae[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_hidden_state\u001b[39m\u001b[38;5;124m'\u001b[39m], attention_mask\u001b[38;5;241m=\u001b[39minput_ae_attn)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'latent_generator' is not defined"
     ]
    }
   ],
   "source": [
    "output_ae = latent_generator.autoencoder.encode(input_ae['last_hidden_state'], attention_mask=input_ae_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.ones(output_ae.shape[0], latent_model.num_encoder_latents, dtype=torch.bool).to(latent_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "def cosine_schedule(t, start = 0, end = 1, tau = 1, clip_min = 1e-9):\n",
    "    power = 2 * tau\n",
    "    v_start = math.cos(start * math.pi / 2) ** power\n",
    "    v_end = math.cos(end * math.pi / 2) ** power\n",
    "    output = torch.cos((t * (end - start) + start) * math.pi / 2) ** power\n",
    "    output = (v_end - output) / (v_end - v_start)\n",
    "    return output.clamp(min = clip_min)\n",
    "\n",
    "def log(t, eps = 1e-12):\n",
    "    return torch.log(t.clamp(min = eps))\n",
    "\n",
    "def log_snr_to_alpha(log_snr):\n",
    "    alpha = torch.sigmoid(log_snr)\n",
    "    return alpha\n",
    "\n",
    "def alpha_to_shifted_log_snr(alpha, scale = 1):\n",
    "    return log((alpha / (1 - alpha))).clamp(min=-15, max=15) + 2*np.log(scale).item()\n",
    "\n",
    "def time_to_alpha(t, alpha_schedule, scale):\n",
    "    alpha = alpha_schedule(t)\n",
    "    shifted_log_snr = alpha_to_shifted_log_snr(alpha, scale = scale)\n",
    "    return log_snr_to_alpha(shifted_log_snr)\n",
    "\n",
    "\n",
    "alpha_schedule = cosine_schedule\n",
    "scale = 1.\n",
    "train_schedule = partial(time_to_alpha, alpha_schedule=alpha_schedule, scale=scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffusion_model.diffusion_model.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 32, 64]), torch.Size([32, 32]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ae.shape, mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_input = {\n",
    "    'txt_latent': output_ae,\n",
    "    'mask': mask,\n",
    "    'class_id': 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_3 = summary(diffusion_model, input_data=diffusion_input, device='cuda', depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('diffusion_model_summary.txt', 'w') as f:\n",
    "    f.write(str(result_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_4 = summary(latent_model, input_data={**i}, device='cuda', depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('latent_model_summary.txt', 'w') as f:\n",
    "    f.write(str(result_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = summary(latent_model.get_encode(), input_data={**i}, device='cuda', depth=10)\n",
    "input_ae = latent_model.get_encoder()(input_ids=i['input_ids'], attention_mask=i['attention_mask'])\n",
    "input_ae_attn = i['attention_mask']\n",
    "result_5 = summary(latent_model.perceiver_ae, input_data={'encoder_outputs': input_ae['last_hidden_state'], 'attention_mask': input_ae_attn}, device='cuda', depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('latent_model_ae_summary.txt', 'w') as f:\n",
    "    f.write(str(result_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 23, 1, 1, 1, 1, 1]), 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.ones(10, 23)\n",
    "padding_dims = 5\n",
    "t.view(*t.shape, *((1,) * padding_dims)).shape, t.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32]),\n",
       " tensor([0.9520, 0.3880, 0.8722, 0.4655, 0.3569, 0.3600, 0.0963, 0.3076, 0.2223,\n",
       "         0.1012, 0.7705, 0.4032, 0.8887, 0.2333, 0.4363, 0.0963, 0.3238, 0.1077,\n",
       "         0.2151, 0.5173, 0.8530, 0.4075, 0.9194, 0.4000, 0.6558, 0.3870, 0.0626,\n",
       "         0.8081, 0.0774, 0.8674, 0.9108, 0.4426]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = 32\n",
    "times = torch.zeros((batch,)).float().uniform_(0, 1.)\n",
    "times.shape, times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32, 64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# noise = torch.randn_like(output_ae)\n",
    "noise = torch.randn((32,32,64))\n",
    "noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32]),\n",
       " tensor([0.0057, 0.6723, 0.0398, 0.5541, 0.7173, 0.7129, 0.9773, 0.7841, 0.8830,\n",
       "         0.9749, 0.1245, 0.6497, 0.0303, 0.8716, 0.5994, 0.9773, 0.7628, 0.9717,\n",
       "         0.8901, 0.4728, 0.0524, 0.6432, 0.0160, 0.6545, 0.2649, 0.6737, 0.9904,\n",
       "         0.0881, 0.9853, 0.0428, 0.0195, 0.5897]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = diffusion_model.train_schedule(times)\n",
    "alpha.shape, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 1])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def right_pad_dims_to(x, t):\n",
    "    padding_dims = x.ndim - t.ndim\n",
    "    if padding_dims <= 0:\n",
    "        return t\n",
    "    return t.view(*t.shape, *((1,) * padding_dims))\n",
    "alpha = right_pad_dims_to(output_ae, alpha)\n",
    "alpha.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = noise.to('cuda')\n",
    "alpha = alpha.to('cuda')\n",
    "output_ae = output_ae.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32, 64])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_t = alpha.sqrt() * output_ae + (1-alpha).sqrt() * noise\n",
    "z_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "from einops import repeat\n",
    "\n",
    "def get_sampling_timesteps(self, batch, *, device, invert = False):\n",
    "    times = torch.linspace(1., 0., self.sampling_timesteps + 1, device = device)\n",
    "    if invert:\n",
    "        times = times.flip(dims = (0,))\n",
    "    times = repeat(times, 't -> b t', b = batch)\n",
    "    times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n",
    "    times = times.unbind(dim = -1)\n",
    "    return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([251])\n",
      "torch.Size([32, 251])\n",
      "torch.Size([2, 32, 250])\n",
      "250 torch.Size([2, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [0.9960, 0.9960, 0.9960, 0.9960, 0.9960, 0.9960, 0.9960, 0.9960, 0.9960,\n",
       "         0.9960, 0.9960, 0.9960, 0.9960, 0.9960, 0.9960, 0.9960, 0.9960, 0.9960,\n",
       "         0.9960, 0.9960, 0.9960, 0.9960, 0.9960, 0.9960, 0.9960, 0.9960, 0.9960,\n",
       "         0.9960, 0.9960, 0.9960, 0.9960, 0.9960]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_timesteps = 250\n",
    "batch = 32\n",
    "\n",
    "times = torch.linspace(1., 0., sampling_timesteps + 1)\n",
    "# times = times.flip(dims = (0,))\n",
    "print(times.shape)\n",
    "times = repeat(times, 't -> b t', b = batch)\n",
    "print(times.shape)\n",
    "times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n",
    "print(times.shape)\n",
    "times = times.unbind(dim = -1)\n",
    "print(len(times), times[0].shape)\n",
    "times[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_schedule(t, start = 0, end = 1, tau = 1, clip_min = 1e-9):\n",
    "    power = 2 * tau\n",
    "    v_start = math.cos(start * math.pi / 2) ** power\n",
    "    v_end = math.cos(end * math.pi / 2) ** power\n",
    "    output = torch.cos((t * (end - start) + start) * math.pi / 2) ** power\n",
    "    output = (v_end - output) / (v_end - v_start)\n",
    "    return output.clamp(min = clip_min)\n",
    "\n",
    "def log(t, eps = 1e-12):\n",
    "    return torch.log(t.clamp(min = eps))\n",
    "\n",
    "def log_snr_to_alpha(log_snr):\n",
    "    alpha = torch.sigmoid(log_snr)\n",
    "    return alpha\n",
    "\n",
    "def alpha_to_shifted_log_snr(alpha, scale = 1):\n",
    "    return log((alpha / (1 - alpha))).clamp(min=-15, max=15) + 2*np.log(scale).item()\n",
    "\n",
    "def time_to_alpha(t, alpha_schedule, scale):\n",
    "    alpha = alpha_schedule(t)\n",
    "    shifted_log_snr = alpha_to_shifted_log_snr(alpha, scale = scale)\n",
    "    return log_snr_to_alpha(shifted_log_snr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [cosine_schedule(torch.tensor(t*(1e-1)), tau=1) for t in range(10)]\n",
    "shifted_log_snr = [alpha_to_shifted_log_snr(a, scale=1) for a in alpha]\n",
    "shifted_log_snr_to_alpha = [log_snr_to_alpha(s) for s in shifted_log_snr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(1.),\n",
       " tensor(0.9755),\n",
       " tensor(0.9045),\n",
       " tensor(0.7939),\n",
       " tensor(0.6545),\n",
       " tensor(0.5000),\n",
       " tensor(0.3455),\n",
       " tensor(0.2061),\n",
       " tensor(0.0955),\n",
       " tensor(0.0245)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(15.),\n",
       " tensor(3.6855),\n",
       " tensor(2.2484),\n",
       " tensor(1.3486),\n",
       " tensor(0.6389),\n",
       " tensor(-5.9605e-08),\n",
       " tensor(-0.6389),\n",
       " tensor(-1.3486),\n",
       " tensor(-2.2484),\n",
       " tensor(-3.6855)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shifted_log_snr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(1.0000),\n",
       " tensor(0.9755),\n",
       " tensor(0.9045),\n",
       " tensor(0.7939),\n",
       " tensor(0.6545),\n",
       " tensor(0.5000),\n",
       " tensor(0.3455),\n",
       " tensor(0.2061),\n",
       " tensor(0.0955),\n",
       " tensor(0.0245)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shifted_log_snr_to_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0251)\n",
      "tensor(1.0785)\n",
      "tensor(1.1393)\n",
      "tensor(1.2130)\n",
      "tensor(1.3090)\n",
      "tensor(1.4472)\n",
      "tensor(1.6763)\n",
      "tensor(2.1584)\n",
      "tensor(3.9021)\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    print(alpha[i]/alpha[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0268, 0.0535, 0.0803, 0.1070, 0.1338, 0.1605, 0.1873, 0.2140, 0.2408,\n",
       "        0.2675, 0.2943, 0.3210, 0.3478, 0.3745, 0.4013, 0.4280, 0.4548, 0.4815,\n",
       "        0.5083, 0.5350, 0.5618, 0.5885, 0.6153, 0.6420, 0.6688, 0.6955, 0.7223,\n",
       "        0.7490, 0.7758, 0.8025, 0.8293, 0.8560, 0.8828, 0.9095, 0.9363, 0.9630,\n",
       "        0.9898, 1.0165, 1.0433, 1.0700, 1.0968, 1.1235, 1.1503, 1.1770, 1.2038,\n",
       "        1.2305, 1.2573, 1.2840, 1.3108, 1.3375, 1.3643, 1.3910, 1.4178, 1.4445,\n",
       "        1.4713, 1.4980, 1.5248, 1.5515, 1.5783, 1.6050, 1.6318, 1.6585, 1.6853,\n",
       "        1.7120])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "# latents = torch.ones((32, 32, 64))\n",
    "latents = torch.arange(1, 65).reshape(1,1,64).repeat(32,32,1).float()\n",
    "# latents[0,0,:]\n",
    "# F.normalize(latents, dim=-1, p=2)[0,0,:]\n",
    "# torch.sum(F.normalize(latents, dim=-1)[0,0,:])\n",
    "# math.sqrt(latents.shape[-1])\n",
    "F.normalize(latents, dim=-1)[0,0,:] * math.sqrt(latents.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (latents / torch.sqrt(torch.sum(latents**2, dim=-1))[0,0])[0,0,:]\n",
    "# torch.sqrt(torch.sum(F.normalize(latents, dim=-1, p=2)[0,0,:]**2))\n",
    "torch.sqrt(torch.sum((F.normalize(latents, dim=-1)[0,0,:] * math.sqrt(latents.shape[-1]))**2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edison",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
