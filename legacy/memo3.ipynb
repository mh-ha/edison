{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maengmaengeeee/anaconda3/envs/edison/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/maengmaengeeee/anaconda3/envs/edison/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "base_model = AutoModel.from_pretrained('microsoft/deberta-v3-xsmall')\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-xsmall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "disc_state_dict = torch.load('weights/disc_xsmall.bin')\n",
    "gen_state_dict = torch.load('weights/gen_xsmall.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deberta.embeddings.word_embeddings._weight: torch.Size([128100, 384])\n",
      "deberta.embeddings.word_embeddings.weight: torch.Size([128100, 384])\n",
      "deberta.embeddings.position_embeddings._weight: torch.Size([512, 384])\n",
      "deberta.embeddings.position_embeddings.weight: torch.Size([512, 384])\n",
      "deberta.embeddings.LayerNorm.weight: torch.Size([384])\n",
      "deberta.embeddings.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.0.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.0.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.0.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.0.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.0.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.0.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.1.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.1.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.1.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.1.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.1.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.1.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.2.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.2.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.2.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.2.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.2.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.2.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.3.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.3.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.3.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.3.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.3.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.3.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.4.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.4.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.4.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.4.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.4.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.4.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.5.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.5.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.5.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.5.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.5.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.5.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.6.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.6.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.6.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.6.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.6.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.6.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.7.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.7.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.7.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.7.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.7.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.7.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.8.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.8.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.8.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.8.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.8.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.8.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.9.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.9.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.9.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.9.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.9.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.9.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.10.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.10.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.10.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.10.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.10.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.10.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.11.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.11.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.11.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.11.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.11.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.11.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.rel_embeddings.weight: torch.Size([512, 384])\n",
      "deberta.encoder.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.LayerNorm.bias: torch.Size([384])\n",
      "lm_predictions.lm_head.bias: torch.Size([128100])\n",
      "lm_predictions.lm_head.dense.weight: torch.Size([384, 384])\n",
      "lm_predictions.lm_head.dense.bias: torch.Size([384])\n",
      "lm_predictions.lm_head.LayerNorm.weight: torch.Size([384])\n",
      "lm_predictions.lm_head.LayerNorm.bias: torch.Size([384])\n",
      "mask_predictions.dense.weight: torch.Size([384, 384])\n",
      "mask_predictions.dense.bias: torch.Size([384])\n",
      "mask_predictions.LayerNorm.weight: torch.Size([384])\n",
      "mask_predictions.LayerNorm.bias: torch.Size([384])\n",
      "mask_predictions.classifier.weight: torch.Size([1, 384])\n",
      "mask_predictions.classifier.bias: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for key, value in disc_state_dict.items():\n",
    "    print(f\"{key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deberta.embeddings.word_embeddings.weight: torch.Size([128100, 384])\n",
      "deberta.embeddings.position_embeddings.weight: torch.Size([512, 384])\n",
      "deberta.embeddings.LayerNorm.weight: torch.Size([384])\n",
      "deberta.embeddings.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.0.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.0.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.0.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.0.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.0.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.0.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.1.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.1.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.1.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.1.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.1.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.1.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.2.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.2.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.2.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.2.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.2.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.2.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.3.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.3.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.3.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.3.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.3.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.3.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.4.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.4.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.4.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.4.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.4.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.4.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.self.query_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.self.key_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.self.value_proj.bias: torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.output.dense.weight: torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.layer.5.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "deberta.encoder.layer.5.intermediate.dense.bias: torch.Size([1536])\n",
      "deberta.encoder.layer.5.output.dense.weight: torch.Size([384, 1536])\n",
      "deberta.encoder.layer.5.output.dense.bias: torch.Size([384])\n",
      "deberta.encoder.layer.5.output.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.layer.5.output.LayerNorm.bias: torch.Size([384])\n",
      "deberta.encoder.rel_embeddings.weight: torch.Size([512, 384])\n",
      "deberta.encoder.LayerNorm.weight: torch.Size([384])\n",
      "deberta.encoder.LayerNorm.bias: torch.Size([384])\n",
      "lm_predictions.lm_head.bias: torch.Size([128100])\n",
      "lm_predictions.lm_head.dense.weight: torch.Size([384, 384])\n",
      "lm_predictions.lm_head.dense.bias: torch.Size([384])\n",
      "lm_predictions.lm_head.LayerNorm.weight: torch.Size([384])\n",
      "lm_predictions.lm_head.LayerNorm.bias: torch.Size([384])\n"
     ]
    }
   ],
   "source": [
    "for key, value in gen_state_dict.items():\n",
    "    print(f\"{key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight: torch.Size([128100, 384])\n",
      "embeddings.LayerNorm.weight: torch.Size([384])\n",
      "embeddings.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.0.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.0.attention.self.query_proj.bias: torch.Size([384])\n",
      "encoder.layer.0.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.0.attention.self.key_proj.bias: torch.Size([384])\n",
      "encoder.layer.0.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.0.attention.self.value_proj.bias: torch.Size([384])\n",
      "encoder.layer.0.attention.output.dense.weight: torch.Size([384, 384])\n",
      "encoder.layer.0.attention.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.0.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.0.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.0.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "encoder.layer.0.intermediate.dense.bias: torch.Size([1536])\n",
      "encoder.layer.0.output.dense.weight: torch.Size([384, 1536])\n",
      "encoder.layer.0.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.0.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.0.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.1.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.1.attention.self.query_proj.bias: torch.Size([384])\n",
      "encoder.layer.1.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.1.attention.self.key_proj.bias: torch.Size([384])\n",
      "encoder.layer.1.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.1.attention.self.value_proj.bias: torch.Size([384])\n",
      "encoder.layer.1.attention.output.dense.weight: torch.Size([384, 384])\n",
      "encoder.layer.1.attention.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.1.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.1.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.1.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "encoder.layer.1.intermediate.dense.bias: torch.Size([1536])\n",
      "encoder.layer.1.output.dense.weight: torch.Size([384, 1536])\n",
      "encoder.layer.1.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.1.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.1.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.2.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.2.attention.self.query_proj.bias: torch.Size([384])\n",
      "encoder.layer.2.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.2.attention.self.key_proj.bias: torch.Size([384])\n",
      "encoder.layer.2.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.2.attention.self.value_proj.bias: torch.Size([384])\n",
      "encoder.layer.2.attention.output.dense.weight: torch.Size([384, 384])\n",
      "encoder.layer.2.attention.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.2.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.2.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.2.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "encoder.layer.2.intermediate.dense.bias: torch.Size([1536])\n",
      "encoder.layer.2.output.dense.weight: torch.Size([384, 1536])\n",
      "encoder.layer.2.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.2.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.2.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.3.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.3.attention.self.query_proj.bias: torch.Size([384])\n",
      "encoder.layer.3.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.3.attention.self.key_proj.bias: torch.Size([384])\n",
      "encoder.layer.3.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.3.attention.self.value_proj.bias: torch.Size([384])\n",
      "encoder.layer.3.attention.output.dense.weight: torch.Size([384, 384])\n",
      "encoder.layer.3.attention.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.3.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.3.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.3.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "encoder.layer.3.intermediate.dense.bias: torch.Size([1536])\n",
      "encoder.layer.3.output.dense.weight: torch.Size([384, 1536])\n",
      "encoder.layer.3.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.3.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.3.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.4.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.4.attention.self.query_proj.bias: torch.Size([384])\n",
      "encoder.layer.4.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.4.attention.self.key_proj.bias: torch.Size([384])\n",
      "encoder.layer.4.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.4.attention.self.value_proj.bias: torch.Size([384])\n",
      "encoder.layer.4.attention.output.dense.weight: torch.Size([384, 384])\n",
      "encoder.layer.4.attention.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.4.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.4.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.4.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "encoder.layer.4.intermediate.dense.bias: torch.Size([1536])\n",
      "encoder.layer.4.output.dense.weight: torch.Size([384, 1536])\n",
      "encoder.layer.4.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.4.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.4.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.5.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.5.attention.self.query_proj.bias: torch.Size([384])\n",
      "encoder.layer.5.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.5.attention.self.key_proj.bias: torch.Size([384])\n",
      "encoder.layer.5.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.5.attention.self.value_proj.bias: torch.Size([384])\n",
      "encoder.layer.5.attention.output.dense.weight: torch.Size([384, 384])\n",
      "encoder.layer.5.attention.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.5.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.5.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.5.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "encoder.layer.5.intermediate.dense.bias: torch.Size([1536])\n",
      "encoder.layer.5.output.dense.weight: torch.Size([384, 1536])\n",
      "encoder.layer.5.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.5.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.5.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.6.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.6.attention.self.query_proj.bias: torch.Size([384])\n",
      "encoder.layer.6.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.6.attention.self.key_proj.bias: torch.Size([384])\n",
      "encoder.layer.6.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.6.attention.self.value_proj.bias: torch.Size([384])\n",
      "encoder.layer.6.attention.output.dense.weight: torch.Size([384, 384])\n",
      "encoder.layer.6.attention.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.6.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.6.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.6.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "encoder.layer.6.intermediate.dense.bias: torch.Size([1536])\n",
      "encoder.layer.6.output.dense.weight: torch.Size([384, 1536])\n",
      "encoder.layer.6.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.6.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.6.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.7.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.7.attention.self.query_proj.bias: torch.Size([384])\n",
      "encoder.layer.7.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.7.attention.self.key_proj.bias: torch.Size([384])\n",
      "encoder.layer.7.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.7.attention.self.value_proj.bias: torch.Size([384])\n",
      "encoder.layer.7.attention.output.dense.weight: torch.Size([384, 384])\n",
      "encoder.layer.7.attention.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.7.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.7.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.7.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "encoder.layer.7.intermediate.dense.bias: torch.Size([1536])\n",
      "encoder.layer.7.output.dense.weight: torch.Size([384, 1536])\n",
      "encoder.layer.7.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.7.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.7.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.8.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.8.attention.self.query_proj.bias: torch.Size([384])\n",
      "encoder.layer.8.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.8.attention.self.key_proj.bias: torch.Size([384])\n",
      "encoder.layer.8.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.8.attention.self.value_proj.bias: torch.Size([384])\n",
      "encoder.layer.8.attention.output.dense.weight: torch.Size([384, 384])\n",
      "encoder.layer.8.attention.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.8.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.8.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.8.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "encoder.layer.8.intermediate.dense.bias: torch.Size([1536])\n",
      "encoder.layer.8.output.dense.weight: torch.Size([384, 1536])\n",
      "encoder.layer.8.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.8.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.8.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.9.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.9.attention.self.query_proj.bias: torch.Size([384])\n",
      "encoder.layer.9.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.9.attention.self.key_proj.bias: torch.Size([384])\n",
      "encoder.layer.9.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.9.attention.self.value_proj.bias: torch.Size([384])\n",
      "encoder.layer.9.attention.output.dense.weight: torch.Size([384, 384])\n",
      "encoder.layer.9.attention.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.9.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.9.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.9.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "encoder.layer.9.intermediate.dense.bias: torch.Size([1536])\n",
      "encoder.layer.9.output.dense.weight: torch.Size([384, 1536])\n",
      "encoder.layer.9.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.9.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.9.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.10.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.10.attention.self.query_proj.bias: torch.Size([384])\n",
      "encoder.layer.10.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.10.attention.self.key_proj.bias: torch.Size([384])\n",
      "encoder.layer.10.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.10.attention.self.value_proj.bias: torch.Size([384])\n",
      "encoder.layer.10.attention.output.dense.weight: torch.Size([384, 384])\n",
      "encoder.layer.10.attention.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.10.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.10.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.10.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "encoder.layer.10.intermediate.dense.bias: torch.Size([1536])\n",
      "encoder.layer.10.output.dense.weight: torch.Size([384, 1536])\n",
      "encoder.layer.10.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.10.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.10.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.11.attention.self.query_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.11.attention.self.query_proj.bias: torch.Size([384])\n",
      "encoder.layer.11.attention.self.key_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.11.attention.self.key_proj.bias: torch.Size([384])\n",
      "encoder.layer.11.attention.self.value_proj.weight: torch.Size([384, 384])\n",
      "encoder.layer.11.attention.self.value_proj.bias: torch.Size([384])\n",
      "encoder.layer.11.attention.output.dense.weight: torch.Size([384, 384])\n",
      "encoder.layer.11.attention.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.11.attention.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.11.attention.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.layer.11.intermediate.dense.weight: torch.Size([1536, 384])\n",
      "encoder.layer.11.intermediate.dense.bias: torch.Size([1536])\n",
      "encoder.layer.11.output.dense.weight: torch.Size([384, 1536])\n",
      "encoder.layer.11.output.dense.bias: torch.Size([384])\n",
      "encoder.layer.11.output.LayerNorm.weight: torch.Size([384])\n",
      "encoder.layer.11.output.LayerNorm.bias: torch.Size([384])\n",
      "encoder.rel_embeddings.weight: torch.Size([512, 384])\n",
      "encoder.LayerNorm.weight: torch.Size([384])\n",
      "encoder.LayerNorm.bias: torch.Size([384])\n"
     ]
    }
   ],
   "source": [
    "for key, value in base_model.named_parameters():\n",
    "    print(f\"{key}: {value.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maengmaengeeee/anaconda3/envs/edison/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from edison.first.module import LM\n",
    "from edison.config.config import Config\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "config = Config(\n",
    "    hidden_dim=384,\n",
    "    embedding_dim=384,\n",
    "    max_seq_len=512,\n",
    "    padding_idx=0,\n",
    "    vocab_size=128001,\n",
    "    absolute_position_biased_input=True,\n",
    "    num_heads=6,\n",
    "    num_head_dim=64,\n",
    "    layernorm_eps=1e-7,\n",
    "    hidden_dropout_prob=0.1,\n",
    "    num_hidden_layers=12,\n",
    "    device='cuda',\n",
    "    mask_lm_prob=0.15,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')\n",
    "model = LM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "LM                                                           --\n",
       "├─Generator: 1-1                                             --\n",
       "│    └─InputEmbedding: 2-1                                   --\n",
       "│    │    └─Embedding: 3-1                                   49,152,384\n",
       "│    │    └─Embedding: 3-2                                   196,608\n",
       "│    │    └─LayerNorm: 3-3                                   768\n",
       "│    └─RelativePositionEmbedding: 2-2                        --\n",
       "│    │    └─Embedding: 3-4                                   196,608\n",
       "│    │    └─LayerNorm: 3-5                                   768\n",
       "│    └─BaseNetworkForLM: 2-3                                 --\n",
       "│    │    └─ModuleList: 3-6                                  10,646,784\n",
       "│    └─EnhancedMaskDecoder: 2-4                              --\n",
       "│    └─MaskedLanguageModelHead: 2-5                          128,001\n",
       "│    │    └─Linear: 3-7                                      147,840\n",
       "│    │    └─GELU: 3-8                                        --\n",
       "│    │    └─LayerNorm: 3-9                                   768\n",
       "│    └─CrossEntropyLoss: 2-6                                 --\n",
       "├─Discriminator: 1-2                                         --\n",
       "│    └─InputEmbedding: 2-7                                   --\n",
       "│    │    └─Embedding: 3-10                                  49,152,384\n",
       "│    │    └─Embedding: 3-11                                  196,608\n",
       "│    │    └─LayerNorm: 3-12                                  768\n",
       "│    └─RelativePositionEmbedding: 2-8                        --\n",
       "│    │    └─Embedding: 3-13                                  196,608\n",
       "│    │    └─LayerNorm: 3-14                                  768\n",
       "│    └─BaseNetworkForLM: 2-9                                 --\n",
       "│    │    └─ModuleList: 3-15                                 21,293,568\n",
       "│    └─EnhancedMaskDecoder: 2-10                             --\n",
       "│    └─ReplacedTokenDiscriminatorHead: 2-11                  --\n",
       "│    │    └─Linear: 3-16                                     147,840\n",
       "│    │    └─GELU: 3-17                                       --\n",
       "│    │    └─LayerNorm: 3-18                                  768\n",
       "│    │    └─Linear: 3-19                                     385\n",
       "│    └─BCEWithLogitsLoss: 2-12                               --\n",
       "=====================================================================================\n",
       "Total params: 131,460,226\n",
       "Trainable params: 131,460,226\n",
       "Non-trainable params: 0\n",
       "====================================================================================="
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_map = {\n",
    "    'deberta.embeddings.word_embeddings.weight': 'discriminator.embedding.word_embedding_layer.weight',\n",
    "    'deberta.embeddings.word_embeddings._weight': 'discriminator.embedding.word_embedding_layer._weight',\n",
    "    'deberta.embeddings.position_embeddings.weight': 'discriminator.embedding.absolute_position_embedding_layer.weight',\n",
    "    'deberta.embeddings.position_embeddings._weight': 'discriminator.embedding.absolute_position_embedding_layer._weight',\n",
    "    'deberta.embeddings.LayerNorm.weight': 'discriminator.embedding.layernorm.weight',\n",
    "    'deberta.embeddings.LayerNorm.bias': 'discriminator.embedding.layernorm.bias',\n",
    "\n",
    "    'attention.self.query_proj.weight': 'attention.query_layer.weight',\n",
    "    'attention.self.query_proj.bias': 'attention.query_layer.bias',\n",
    "    'attention.self.key_proj.weight': 'attention.key_layer.weight',\n",
    "    'attention.self.key_proj.bias': 'attention.key_layer.bias',\n",
    "    'attention.self.value_proj.weight': 'attention.value_layer.weight',\n",
    "    'attention.self.value_proj.bias': 'attention.value_layer.bias',\n",
    "    'attention.output.dense.weight': 'attention.feedforward.weight',\n",
    "    'attention.output.dense.bias': 'attention.feedforward.bias',\n",
    "    'attention.output.LayerNorm.weight': 'attention.feedforward.layernorm.weight',\n",
    "    'attention.output.LayerNorm.bias': 'attention.feedforward.layernorm.bias',\n",
    "    'intermediate.dense.weight': 'feedforward.feedforward_1.weight',\n",
    "    'intermediate.dense.bias': 'feedforward.feedforward_1.bias',\n",
    "    'output.dense.weight': 'feedforward.feedforward_2.weight',\n",
    "    'output.dense.bias': 'feedforward.feedforward_2.bias',\n",
    "    'output.LayerNorm.weight': 'feedforward.layernorm.weight',\n",
    "    'output.LayerNorm.bias': 'feedforward.layernorm.bias',\n",
    "    \n",
    "    'deberta.encoder.rel_embeddings.weight': 'discriminator.relative_position_embedding.relative_position_embedding_layer.weight',\n",
    "    'deberta.encoder.LayerNorm.weight': 'discriminator.relative_position_embedding.layernorm.weight',\n",
    "    'deberta.encoder.LayerNorm.bias': 'discriminator.relative_position_embedding.layernorm.bias',\n",
    "    'mask_predictions.dense.weight': 'discriminator.head.dense.weight',\n",
    "    'mask_predictions.dense.bias': 'discriminator.head.dense.bias',\n",
    "    'mask_predictions.LayerNorm.weight': 'discriminator.head.layernorm.weight',\n",
    "    'mask_predictions.LayerNorm.bias': 'discriminator.head.layernorm.bias',\n",
    "    'mask_predictions.classifier.weight': 'discriminator.head.classifier.weight',\n",
    "    'mask_predictions.classifier.bias': 'discriminator.head.classifier.bias',\n",
    "    'lm_predictions.lm_head.bias': 'generator.head.bias',\n",
    "    'lm_predictions.lm_head.dense.weight': 'generator.head.dense.weight',\n",
    "    'lm_predictions.lm_head.dense.bias': 'generator.head.dense.bias',\n",
    "    'lm_predictions.lm_head.LayerNorm.weight': 'generator.head.layernorm.weight',\n",
    "    'lm_predictions.lm_head.LayerNorm.bias': 'generator.head.layernorm.bias',\n",
    "}\n",
    "weight_path = 'weights/disc_xsmall.bin'\n",
    "base_state_dict = torch.load(weight_path)\n",
    "disc_state_dict = {}\n",
    "idx = None\n",
    "for k, v in base_state_dict.items():\n",
    "    if k.startswith('deberta.encoder.layer'):\n",
    "        prefix = 'discriminator.encoder.layers'\n",
    "        idx = int(k.split('.')[3])\n",
    "        k = '.'.join(k.split('.')[4:])\n",
    "    if k in layer_map:\n",
    "        k = layer_map[k]\n",
    "    try:\n",
    "        key = f'{prefix}.{idx}.{k}' if idx is not None else k\n",
    "        disc_state_dict[key] = v\n",
    "        idx = None\n",
    "    except KeyError:\n",
    "        print(f'KeyError: {k}')\n",
    "# model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator.embedding.word_embedding_layer._weight: torch.Size([128100, 384])\n",
      "discriminator.embedding.word_embedding_layer.weight: torch.Size([128100, 384])\n",
      "discriminator.embedding.absolute_position_embedding_layer._weight: torch.Size([512, 384])\n",
      "discriminator.embedding.absolute_position_embedding_layer.weight: torch.Size([512, 384])\n",
      "discriminator.embedding.layernorm.weight: torch.Size([384])\n",
      "discriminator.embedding.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.0.attention.query_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.0.attention.query_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.0.attention.key_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.0.attention.key_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.0.attention.value_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.0.attention.value_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.0.attention.feedforward.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.0.attention.feedforward.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.0.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.0.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.0.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "discriminator.encoder.layers.0.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "discriminator.encoder.layers.0.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "discriminator.encoder.layers.0.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.0.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.0.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.1.attention.query_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.1.attention.query_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.1.attention.key_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.1.attention.key_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.1.attention.value_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.1.attention.value_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.1.attention.feedforward.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.1.attention.feedforward.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.1.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.1.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.1.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "discriminator.encoder.layers.1.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "discriminator.encoder.layers.1.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "discriminator.encoder.layers.1.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.1.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.1.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.2.attention.query_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.2.attention.query_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.2.attention.key_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.2.attention.key_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.2.attention.value_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.2.attention.value_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.2.attention.feedforward.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.2.attention.feedforward.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.2.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.2.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.2.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "discriminator.encoder.layers.2.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "discriminator.encoder.layers.2.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "discriminator.encoder.layers.2.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.2.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.2.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.3.attention.query_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.3.attention.query_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.3.attention.key_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.3.attention.key_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.3.attention.value_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.3.attention.value_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.3.attention.feedforward.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.3.attention.feedforward.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.3.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.3.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.3.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "discriminator.encoder.layers.3.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "discriminator.encoder.layers.3.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "discriminator.encoder.layers.3.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.3.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.3.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.4.attention.query_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.4.attention.query_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.4.attention.key_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.4.attention.key_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.4.attention.value_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.4.attention.value_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.4.attention.feedforward.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.4.attention.feedforward.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.4.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.4.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.4.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "discriminator.encoder.layers.4.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "discriminator.encoder.layers.4.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "discriminator.encoder.layers.4.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.4.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.4.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.5.attention.query_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.5.attention.query_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.5.attention.key_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.5.attention.key_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.5.attention.value_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.5.attention.value_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.5.attention.feedforward.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.5.attention.feedforward.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.5.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.5.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.5.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "discriminator.encoder.layers.5.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "discriminator.encoder.layers.5.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "discriminator.encoder.layers.5.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.5.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.5.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.6.attention.query_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.6.attention.query_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.6.attention.key_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.6.attention.key_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.6.attention.value_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.6.attention.value_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.6.attention.feedforward.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.6.attention.feedforward.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.6.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.6.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.6.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "discriminator.encoder.layers.6.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "discriminator.encoder.layers.6.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "discriminator.encoder.layers.6.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.6.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.6.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.7.attention.query_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.7.attention.query_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.7.attention.key_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.7.attention.key_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.7.attention.value_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.7.attention.value_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.7.attention.feedforward.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.7.attention.feedforward.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.7.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.7.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.7.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "discriminator.encoder.layers.7.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "discriminator.encoder.layers.7.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "discriminator.encoder.layers.7.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.7.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.7.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.8.attention.query_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.8.attention.query_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.8.attention.key_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.8.attention.key_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.8.attention.value_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.8.attention.value_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.8.attention.feedforward.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.8.attention.feedforward.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.8.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.8.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.8.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "discriminator.encoder.layers.8.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "discriminator.encoder.layers.8.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "discriminator.encoder.layers.8.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.8.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.8.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.9.attention.query_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.9.attention.query_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.9.attention.key_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.9.attention.key_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.9.attention.value_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.9.attention.value_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.9.attention.feedforward.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.9.attention.feedforward.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.9.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.9.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.9.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "discriminator.encoder.layers.9.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "discriminator.encoder.layers.9.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "discriminator.encoder.layers.9.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.9.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.9.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.10.attention.query_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.10.attention.query_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.10.attention.key_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.10.attention.key_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.10.attention.value_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.10.attention.value_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.10.attention.feedforward.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.10.attention.feedforward.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.10.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.10.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.10.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "discriminator.encoder.layers.10.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "discriminator.encoder.layers.10.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "discriminator.encoder.layers.10.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.10.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.10.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.11.attention.query_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.11.attention.query_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.11.attention.key_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.11.attention.key_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.11.attention.value_layer.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.11.attention.value_layer.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.11.attention.feedforward.weight: torch.Size([384, 384])\n",
      "discriminator.encoder.layers.11.attention.feedforward.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.11.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.11.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.11.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "discriminator.encoder.layers.11.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "discriminator.encoder.layers.11.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "discriminator.encoder.layers.11.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "discriminator.encoder.layers.11.feedforward.layernorm.weight: torch.Size([384])\n",
      "discriminator.encoder.layers.11.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.relative_position_embedding.relative_position_embedding_layer.weight: torch.Size([512, 384])\n",
      "discriminator.relative_position_embedding.layernorm.weight: torch.Size([384])\n",
      "discriminator.relative_position_embedding.layernorm.bias: torch.Size([384])\n",
      "generator.head.bias: torch.Size([128100])\n",
      "generator.head.dense.weight: torch.Size([384, 384])\n",
      "generator.head.dense.bias: torch.Size([384])\n",
      "generator.head.layernorm.weight: torch.Size([384])\n",
      "generator.head.layernorm.bias: torch.Size([384])\n",
      "discriminator.head.dense.weight: torch.Size([384, 384])\n",
      "discriminator.head.dense.bias: torch.Size([384])\n",
      "discriminator.head.layernorm.weight: torch.Size([384])\n",
      "discriminator.head.layernorm.bias: torch.Size([384])\n",
      "discriminator.head.classifier.weight: torch.Size([1, 384])\n",
      "discriminator.head.classifier.bias: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for key, value in disc_state_dict.items():\n",
    "    print(f\"{key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m weight_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights/gen_xsmall.bin\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m base_state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mload(weight_path)  \u001b[38;5;66;03m# gen\u001b[39;00m\n\u001b[1;32m      3\u001b[0m gen_state_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      4\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "weight_path = 'weights/gen_xsmall.bin'\n",
    "base_state_dict = torch.load(weight_path)  # gen\n",
    "gen_state_dict = {}\n",
    "idx = None\n",
    "for k, v in base_state_dict.items():\n",
    "    if k.startswith('deberta.encoder.layer'):\n",
    "        prefix = 'generator.encoder.layers'\n",
    "        idx = int(k.split('.')[3])\n",
    "        k = '.'.join(k.split('.')[4:])\n",
    "    if k in layer_map:\n",
    "        k = layer_map[k]\n",
    "    try:\n",
    "        key = f'{prefix}.{idx}.{k}' if idx is not None else k\n",
    "        gen_state_dict[key] = v\n",
    "        idx = None\n",
    "    except KeyError:\n",
    "        print(f'KeyError: {k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator.embedding.word_embedding_layer.weight: torch.Size([128100, 384])\n",
      "discriminator.embedding.absolute_position_embedding_layer.weight: torch.Size([512, 384])\n",
      "discriminator.embedding.layernorm.weight: torch.Size([384])\n",
      "discriminator.embedding.layernorm.bias: torch.Size([384])\n",
      "generator.encoder.layers.0.attention.query_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.0.attention.query_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.0.attention.key_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.0.attention.key_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.0.attention.value_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.0.attention.value_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.0.attention.feedforward.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.0.attention.feedforward.bias: torch.Size([384])\n",
      "generator.encoder.layers.0.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "generator.encoder.layers.0.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "generator.encoder.layers.0.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "generator.encoder.layers.0.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "generator.encoder.layers.0.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "generator.encoder.layers.0.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "generator.encoder.layers.0.feedforward.layernorm.weight: torch.Size([384])\n",
      "generator.encoder.layers.0.feedforward.layernorm.bias: torch.Size([384])\n",
      "generator.encoder.layers.1.attention.query_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.1.attention.query_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.1.attention.key_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.1.attention.key_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.1.attention.value_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.1.attention.value_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.1.attention.feedforward.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.1.attention.feedforward.bias: torch.Size([384])\n",
      "generator.encoder.layers.1.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "generator.encoder.layers.1.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "generator.encoder.layers.1.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "generator.encoder.layers.1.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "generator.encoder.layers.1.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "generator.encoder.layers.1.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "generator.encoder.layers.1.feedforward.layernorm.weight: torch.Size([384])\n",
      "generator.encoder.layers.1.feedforward.layernorm.bias: torch.Size([384])\n",
      "generator.encoder.layers.2.attention.query_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.2.attention.query_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.2.attention.key_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.2.attention.key_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.2.attention.value_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.2.attention.value_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.2.attention.feedforward.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.2.attention.feedforward.bias: torch.Size([384])\n",
      "generator.encoder.layers.2.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "generator.encoder.layers.2.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "generator.encoder.layers.2.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "generator.encoder.layers.2.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "generator.encoder.layers.2.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "generator.encoder.layers.2.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "generator.encoder.layers.2.feedforward.layernorm.weight: torch.Size([384])\n",
      "generator.encoder.layers.2.feedforward.layernorm.bias: torch.Size([384])\n",
      "generator.encoder.layers.3.attention.query_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.3.attention.query_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.3.attention.key_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.3.attention.key_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.3.attention.value_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.3.attention.value_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.3.attention.feedforward.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.3.attention.feedforward.bias: torch.Size([384])\n",
      "generator.encoder.layers.3.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "generator.encoder.layers.3.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "generator.encoder.layers.3.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "generator.encoder.layers.3.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "generator.encoder.layers.3.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "generator.encoder.layers.3.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "generator.encoder.layers.3.feedforward.layernorm.weight: torch.Size([384])\n",
      "generator.encoder.layers.3.feedforward.layernorm.bias: torch.Size([384])\n",
      "generator.encoder.layers.4.attention.query_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.4.attention.query_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.4.attention.key_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.4.attention.key_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.4.attention.value_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.4.attention.value_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.4.attention.feedforward.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.4.attention.feedforward.bias: torch.Size([384])\n",
      "generator.encoder.layers.4.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "generator.encoder.layers.4.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "generator.encoder.layers.4.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "generator.encoder.layers.4.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "generator.encoder.layers.4.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "generator.encoder.layers.4.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "generator.encoder.layers.4.feedforward.layernorm.weight: torch.Size([384])\n",
      "generator.encoder.layers.4.feedforward.layernorm.bias: torch.Size([384])\n",
      "generator.encoder.layers.5.attention.query_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.5.attention.query_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.5.attention.key_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.5.attention.key_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.5.attention.value_layer.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.5.attention.value_layer.bias: torch.Size([384])\n",
      "generator.encoder.layers.5.attention.feedforward.weight: torch.Size([384, 384])\n",
      "generator.encoder.layers.5.attention.feedforward.bias: torch.Size([384])\n",
      "generator.encoder.layers.5.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "generator.encoder.layers.5.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "generator.encoder.layers.5.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "generator.encoder.layers.5.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "generator.encoder.layers.5.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "generator.encoder.layers.5.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "generator.encoder.layers.5.feedforward.layernorm.weight: torch.Size([384])\n",
      "generator.encoder.layers.5.feedforward.layernorm.bias: torch.Size([384])\n",
      "discriminator.relative_position_embedding.relative_position_embedding_layer.weight: torch.Size([512, 384])\n",
      "discriminator.relative_position_embedding.layernorm.weight: torch.Size([384])\n",
      "discriminator.relative_position_embedding.layernorm.bias: torch.Size([384])\n",
      "generator.head.bias: torch.Size([128100])\n",
      "generator.head.dense.weight: torch.Size([384, 384])\n",
      "generator.head.dense.bias: torch.Size([384])\n",
      "generator.head.layernorm.weight: torch.Size([384])\n",
      "generator.head.layernorm.bias: torch.Size([384])\n"
     ]
    }
   ],
   "source": [
    "for key, value in gen_state_dict.items():\n",
    "    print(f\"{key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maengmaengeeee/anaconda3/envs/edison/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not included: deberta.embeddings.word_embeddings.weight\n",
      "not included: deberta.embeddings.position_embeddings.weight\n",
      "not included: lm_predictions.lm_head.bias\n",
      "not included: lm_predictions.lm_head.dense.weight\n",
      "not included: lm_predictions.lm_head.dense.bias\n",
      "not included: lm_predictions.lm_head.LayerNorm.weight\n",
      "not included: lm_predictions.lm_head.LayerNorm.bias\n"
     ]
    }
   ],
   "source": [
    "from edison.first.load_state import load_pretrained_LM\n",
    "\n",
    "model, disc, gen = load_pretrained_LM(\n",
    "    ['weights/disc_xsmall.bin', 'weights/gen_xsmall.bin'],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.word_embedding_layer._weight: torch.Size([128100, 384])\n",
      "embedding.absolute_position_embedding_layer._weight: torch.Size([512, 384])\n",
      "embedding.layernorm.weight: torch.Size([384])\n",
      "embedding.layernorm.bias: torch.Size([384])\n",
      "relative_position_embedding.relative_position_embedding_layer.weight: torch.Size([512, 384])\n",
      "relative_position_embedding.layernorm.weight: torch.Size([384])\n",
      "relative_position_embedding.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.0.attention.query_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.0.attention.query_layer.bias: torch.Size([384])\n",
      "encoder.layers.0.attention.key_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.0.attention.key_layer.bias: torch.Size([384])\n",
      "encoder.layers.0.attention.value_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.0.attention.value_layer.bias: torch.Size([384])\n",
      "encoder.layers.0.attention.feedforward.dense.weight: torch.Size([384, 384])\n",
      "encoder.layers.0.attention.feedforward.dense.bias: torch.Size([384])\n",
      "encoder.layers.0.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.0.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.0.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "encoder.layers.0.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "encoder.layers.0.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "encoder.layers.0.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "encoder.layers.0.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.0.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.1.attention.query_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.1.attention.query_layer.bias: torch.Size([384])\n",
      "encoder.layers.1.attention.key_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.1.attention.key_layer.bias: torch.Size([384])\n",
      "encoder.layers.1.attention.value_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.1.attention.value_layer.bias: torch.Size([384])\n",
      "encoder.layers.1.attention.feedforward.dense.weight: torch.Size([384, 384])\n",
      "encoder.layers.1.attention.feedforward.dense.bias: torch.Size([384])\n",
      "encoder.layers.1.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.1.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.1.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "encoder.layers.1.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "encoder.layers.1.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "encoder.layers.1.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "encoder.layers.1.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.1.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.2.attention.query_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.2.attention.query_layer.bias: torch.Size([384])\n",
      "encoder.layers.2.attention.key_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.2.attention.key_layer.bias: torch.Size([384])\n",
      "encoder.layers.2.attention.value_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.2.attention.value_layer.bias: torch.Size([384])\n",
      "encoder.layers.2.attention.feedforward.dense.weight: torch.Size([384, 384])\n",
      "encoder.layers.2.attention.feedforward.dense.bias: torch.Size([384])\n",
      "encoder.layers.2.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.2.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.2.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "encoder.layers.2.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "encoder.layers.2.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "encoder.layers.2.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "encoder.layers.2.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.2.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.3.attention.query_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.3.attention.query_layer.bias: torch.Size([384])\n",
      "encoder.layers.3.attention.key_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.3.attention.key_layer.bias: torch.Size([384])\n",
      "encoder.layers.3.attention.value_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.3.attention.value_layer.bias: torch.Size([384])\n",
      "encoder.layers.3.attention.feedforward.dense.weight: torch.Size([384, 384])\n",
      "encoder.layers.3.attention.feedforward.dense.bias: torch.Size([384])\n",
      "encoder.layers.3.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.3.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.3.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "encoder.layers.3.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "encoder.layers.3.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "encoder.layers.3.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "encoder.layers.3.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.3.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.4.attention.query_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.4.attention.query_layer.bias: torch.Size([384])\n",
      "encoder.layers.4.attention.key_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.4.attention.key_layer.bias: torch.Size([384])\n",
      "encoder.layers.4.attention.value_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.4.attention.value_layer.bias: torch.Size([384])\n",
      "encoder.layers.4.attention.feedforward.dense.weight: torch.Size([384, 384])\n",
      "encoder.layers.4.attention.feedforward.dense.bias: torch.Size([384])\n",
      "encoder.layers.4.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.4.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.4.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "encoder.layers.4.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "encoder.layers.4.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "encoder.layers.4.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "encoder.layers.4.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.4.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.5.attention.query_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.5.attention.query_layer.bias: torch.Size([384])\n",
      "encoder.layers.5.attention.key_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.5.attention.key_layer.bias: torch.Size([384])\n",
      "encoder.layers.5.attention.value_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.5.attention.value_layer.bias: torch.Size([384])\n",
      "encoder.layers.5.attention.feedforward.dense.weight: torch.Size([384, 384])\n",
      "encoder.layers.5.attention.feedforward.dense.bias: torch.Size([384])\n",
      "encoder.layers.5.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.5.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.5.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "encoder.layers.5.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "encoder.layers.5.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "encoder.layers.5.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "encoder.layers.5.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.5.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.6.attention.query_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.6.attention.query_layer.bias: torch.Size([384])\n",
      "encoder.layers.6.attention.key_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.6.attention.key_layer.bias: torch.Size([384])\n",
      "encoder.layers.6.attention.value_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.6.attention.value_layer.bias: torch.Size([384])\n",
      "encoder.layers.6.attention.feedforward.dense.weight: torch.Size([384, 384])\n",
      "encoder.layers.6.attention.feedforward.dense.bias: torch.Size([384])\n",
      "encoder.layers.6.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.6.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.6.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "encoder.layers.6.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "encoder.layers.6.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "encoder.layers.6.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "encoder.layers.6.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.6.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.7.attention.query_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.7.attention.query_layer.bias: torch.Size([384])\n",
      "encoder.layers.7.attention.key_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.7.attention.key_layer.bias: torch.Size([384])\n",
      "encoder.layers.7.attention.value_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.7.attention.value_layer.bias: torch.Size([384])\n",
      "encoder.layers.7.attention.feedforward.dense.weight: torch.Size([384, 384])\n",
      "encoder.layers.7.attention.feedforward.dense.bias: torch.Size([384])\n",
      "encoder.layers.7.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.7.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.7.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "encoder.layers.7.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "encoder.layers.7.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "encoder.layers.7.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "encoder.layers.7.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.7.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.8.attention.query_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.8.attention.query_layer.bias: torch.Size([384])\n",
      "encoder.layers.8.attention.key_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.8.attention.key_layer.bias: torch.Size([384])\n",
      "encoder.layers.8.attention.value_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.8.attention.value_layer.bias: torch.Size([384])\n",
      "encoder.layers.8.attention.feedforward.dense.weight: torch.Size([384, 384])\n",
      "encoder.layers.8.attention.feedforward.dense.bias: torch.Size([384])\n",
      "encoder.layers.8.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.8.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.8.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "encoder.layers.8.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "encoder.layers.8.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "encoder.layers.8.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "encoder.layers.8.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.8.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.9.attention.query_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.9.attention.query_layer.bias: torch.Size([384])\n",
      "encoder.layers.9.attention.key_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.9.attention.key_layer.bias: torch.Size([384])\n",
      "encoder.layers.9.attention.value_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.9.attention.value_layer.bias: torch.Size([384])\n",
      "encoder.layers.9.attention.feedforward.dense.weight: torch.Size([384, 384])\n",
      "encoder.layers.9.attention.feedforward.dense.bias: torch.Size([384])\n",
      "encoder.layers.9.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.9.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.9.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "encoder.layers.9.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "encoder.layers.9.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "encoder.layers.9.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "encoder.layers.9.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.9.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.10.attention.query_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.10.attention.query_layer.bias: torch.Size([384])\n",
      "encoder.layers.10.attention.key_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.10.attention.key_layer.bias: torch.Size([384])\n",
      "encoder.layers.10.attention.value_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.10.attention.value_layer.bias: torch.Size([384])\n",
      "encoder.layers.10.attention.feedforward.dense.weight: torch.Size([384, 384])\n",
      "encoder.layers.10.attention.feedforward.dense.bias: torch.Size([384])\n",
      "encoder.layers.10.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.10.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.10.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "encoder.layers.10.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "encoder.layers.10.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "encoder.layers.10.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "encoder.layers.10.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.10.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.11.attention.query_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.11.attention.query_layer.bias: torch.Size([384])\n",
      "encoder.layers.11.attention.key_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.11.attention.key_layer.bias: torch.Size([384])\n",
      "encoder.layers.11.attention.value_layer.weight: torch.Size([384, 384])\n",
      "encoder.layers.11.attention.value_layer.bias: torch.Size([384])\n",
      "encoder.layers.11.attention.feedforward.dense.weight: torch.Size([384, 384])\n",
      "encoder.layers.11.attention.feedforward.dense.bias: torch.Size([384])\n",
      "encoder.layers.11.attention.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.11.attention.feedforward.layernorm.bias: torch.Size([384])\n",
      "encoder.layers.11.feedforward.feedforward_1.weight: torch.Size([1536, 384])\n",
      "encoder.layers.11.feedforward.feedforward_1.bias: torch.Size([1536])\n",
      "encoder.layers.11.feedforward.feedforward_2.weight: torch.Size([384, 1536])\n",
      "encoder.layers.11.feedforward.feedforward_2.bias: torch.Size([384])\n",
      "encoder.layers.11.feedforward.layernorm.weight: torch.Size([384])\n",
      "encoder.layers.11.feedforward.layernorm.bias: torch.Size([384])\n",
      "head.dense.weight: torch.Size([384, 384])\n",
      "head.dense.bias: torch.Size([384])\n",
      "head.layernorm.weight: torch.Size([384])\n",
      "head.layernorm.bias: torch.Size([384])\n",
      "head.classifier.weight: torch.Size([1, 384])\n",
      "head.classifier.bias: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for key, value in model.discriminator.named_parameters():\n",
    "    print(f\"{key}: {value.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('lightning_logs/version_91/metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss_disc</th>\n",
       "      <th>loss_gen</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>step</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "      <td>15.887872</td>\n",
       "      <td>10.360911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0</td>\n",
       "      <td>16.492992</td>\n",
       "      <td>10.419763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0</td>\n",
       "      <td>11.897016</td>\n",
       "      <td>8.268582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0</td>\n",
       "      <td>14.725536</td>\n",
       "      <td>9.164614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0</td>\n",
       "      <td>14.213744</td>\n",
       "      <td>8.794498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183799</th>\n",
       "      <td>0</td>\n",
       "      <td>6.835743</td>\n",
       "      <td>4.313547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183849</th>\n",
       "      <td>0</td>\n",
       "      <td>13.380176</td>\n",
       "      <td>4.591283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183899</th>\n",
       "      <td>0</td>\n",
       "      <td>4.943836</td>\n",
       "      <td>2.098058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183949</th>\n",
       "      <td>0</td>\n",
       "      <td>6.820099</td>\n",
       "      <td>4.028820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183999</th>\n",
       "      <td>0</td>\n",
       "      <td>12.993057</td>\n",
       "      <td>2.609025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3680 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        epoch  loss_disc   loss_gen\n",
       "step                               \n",
       "49          0  15.887872  10.360911\n",
       "99          0  16.492992  10.419763\n",
       "149         0  11.897016   8.268582\n",
       "199         0  14.725536   9.164614\n",
       "249         0  14.213744   8.794498\n",
       "...       ...        ...        ...\n",
       "183799      0   6.835743   4.313547\n",
       "183849      0  13.380176   4.591283\n",
       "183899      0   4.943836   2.098058\n",
       "183949      0   6.820099   4.028820\n",
       "183999      0  12.993057   2.609025\n",
       "\n",
       "[3680 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.set_index('step', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'Loss'}, xlabel='step'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.plot(y=['loss_disc', 'loss_gen'], title='Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edison.first.datamodule import LMDataModule\n",
    "\n",
    "dm = LMDataModule(config=config, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.dataset_names=['wikipedia']\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "dataset = dm.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 28795678\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dm.train_dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(dataset[11000]['text'][:512], truncation=True, max_length=128)\n",
    "len(tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edison",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
